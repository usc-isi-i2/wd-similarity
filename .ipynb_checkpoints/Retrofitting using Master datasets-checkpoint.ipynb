{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dynamic-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import combinations\n",
    "from math import comb\n",
    "from time import time\n",
    "from datetime import date\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "amazing-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDSIM_DF = '../data/evaluation/wordsim353_with_r3.csv'\n",
    "WORDSIM_OLD_FINAL_FILE = \"../data/evaluation/wordsim_old.csv\"\n",
    "CONCEPTNET_FILE = \"../data/evaluation/kgtk_conceptnet_final.csv\"\n",
    "WIKI_CS_FILE = '../data/evaluation/wikidata-cs_final.csv'\n",
    "\n",
    "INPUT_EMB_FOLDER_PATH = '../data/embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "improved-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basis\n",
    "P279_CHILD_PAR_DISTILBERT_COSSIM_FILE = \"../data/basis/P279_ChildPar.all-distilroberta-v1.csv\"\n",
    "P279_SIBLINGS_DISTILBERT_COSSIM_FILE = \"../data/basis/P279_Siblings.all-distilroberta-v1.csv\"\n",
    "\n",
    "P279_CHILD_PAR_CLASSSIM_FILE = \"../data/basis/P279_ChildPar.classSim.csv\"\n",
    "P279_SIBLINGS_CLASSSIM_FILE = \"../data/basis/P279_Siblings.classSim.csv\"\n",
    "\n",
    "PROBASE_FINAL_FILE = '../data/basis/intermediate_files/probase_WQnodes_subset_and_sim.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fallen-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDSIM_CLASS_SIM_FILE = '../data/embeddings/wordsim_class_sim.csv'\n",
    "WORDSIM_JC_SIM_FILE = '../data/embeddings/wordsim_jc_sim.csv'\n",
    "WORDSIM_TOP_SIM_FILE = '../data/embeddings/wordsim_top_sim.csv'\n",
    "\n",
    "WORDSIM_OLD_CLASS_SIM_FILE = '../data/embeddings/wordsim_old_class_sim.csv'\n",
    "WORDSIM_OLD_JC_SIM_FILE = '../data/embeddings/wordsim_old_jc_sim.csv'\n",
    "WORDSIM_OLD_TOP_SIM_FILE = '../data/embeddings/wordsim_old_top_sim.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-conspiracy",
   "metadata": {},
   "source": [
    "# Retrofitting Pre-Req Class Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-perception",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "arbitrary-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    This contains all the utility functions needed by any part of retrofitting\n",
    "    \"\"\"\n",
    "    _today = date.today()\n",
    "    today_date = _today.strftime(\"%b_%d_%Y\")\n",
    "    LABELS = ['I','M','U']\n",
    "    \n",
    "    @classmethod\n",
    "    def normalize(cls, embed_dict):\n",
    "        for key, val in embed_dict.items():\n",
    "            temp = np.array([float(val1) for val1 in val])\n",
    "            temp2 = temp**2\n",
    "            embed_dict[key] = temp / np.sqrt((temp2.sum() + 1e-6))\n",
    "        return embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def fetch_embeddings(cls, df):\n",
    "        embed_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            embed_dict[row.node] = row.value\n",
    "        return normalize(embed_dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def fill_coverage(cls, embed_dict, embed_name):\n",
    "        wordsim_df = pd.read_csv(WORDSIM_DF)\n",
    "#         wiki_cs_df = pd.read_csv(WIKICS_DF)\n",
    "#         concept_net_df = pd.read_csv(CONCEPTNET_DF)\n",
    "        \n",
    "        compulsory_coverage_set = set(\n",
    "                        wordsim_df['word1_kg_id'].to_list() \n",
    "                        + wordsim_df['word2_kg_id'].to_list())\n",
    "#                         + wiki_cs_df['word1_kg_id'].to_list() \n",
    "#                         + wiki_cs_df['word2_kg_id'].to_list()\n",
    "#                         + concept_net_df['word1_kg_id'].to_list()\n",
    "#                         + concept_net_df['word2_kg_id'].to_list())\n",
    "        \n",
    "        embed_size = len(embed_dict[next(iter(embed_dict))])\n",
    "        count = 0\n",
    "        for word in compulsory_coverage_set:\n",
    "            if word not in embed_dict:\n",
    "                embed_dict[word] = np.zeros((embed_size))\n",
    "                count += 1\n",
    "        print(f\"Added {count} corrections to {embed_name}\")\n",
    "        return embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def check_coverage(cls, embed_dict):\n",
    "        wordsim_df = pd.read_csv(WORDSIM_DF)\n",
    "        \n",
    "        compulsory_coverage_set = set(list(zip(wordsim_df['word1_kg_id'].to_list(), wordsim_df['word2_kg_id'].to_list())))\n",
    "        embed_size = len(embed_dict[next(iter(embed_dict))])\n",
    "        count = 0\n",
    "        for word1, word2 in compulsory_coverage_set:\n",
    "            if word1 not in embed_dict or word2 not in embed_dict:\n",
    "                count += 1\n",
    "        return (len(wordsim_df) - count)\n",
    "    \n",
    "    @classmethod\n",
    "    def determine_distances(cls, embed_dict, new_embed_dict):\n",
    "        dist = []\n",
    "        for word in embed_dict.keys():\n",
    "            dist.append(euclidean_distances([embed_dict[word]], [new_embed_dict[word]])[0][0])\n",
    "        return dist\n",
    "    \n",
    "    @classmethod\n",
    "    def serialize_embedding_dict(cls, embed_dict):\n",
    "        for key2 in embed_dict.keys():\n",
    "            embed_dict[key2] = embed_dict[key2].tolist() if type(embed_dict[key2]) != list else embed_dict[key2]\n",
    "        return embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def deserialize_embedding_dict(cls, embed_dict):\n",
    "        for key2 in embed_dict.keys():\n",
    "            embed_dict[key2] = np.array(embed_dict[key2])\n",
    "        return embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def label_samples(cls, score):\n",
    "        return 'I' if score <= 1.75 else 'U' if score >= 3.5 else 'M'\n",
    "    \n",
    "    @classmethod\n",
    "    def alt_label_samples(cls, score, quartiles):\n",
    "        return ['Q'+str(i+1) for i in range(len(quartiles) - 1) if quartiles[i] <= score < quartiles[i+1]][0]\n",
    "    \n",
    "    @classmethod\n",
    "    def alt2_label_samples(cls, row, quartiles):\n",
    "        return [i for i, quartile in (quartiles.items()) if (row.word1_kg_id, row.word2_kg_id) in quartile][0]\n",
    "    \n",
    "    @classmethod\n",
    "    def determine_cos_sim(cls, emb1, emb2):\n",
    "        return cosine_similarity(\n",
    "                np.array(emb1).reshape(1,-1), \n",
    "                np.array(emb2).reshape(1,-1)\n",
    "            )[0][0]\n",
    "    \n",
    "    @classmethod\n",
    "    def plot_confusion_matrix(cls, conf_matrix, title):\n",
    "        plt.close()\n",
    "        sns.heatmap(conf_matrix, xticklabels=Utils.LABELS, yticklabels=Utils.LABELS, annot=True)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(title+' Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "urban-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# def sizeof_fmt(num, suffix='B'):\n",
    "#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "#         if abs(num) < 1024.0:\n",
    "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "#         num /= 1024.0\n",
    "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "#     for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "#                              key= lambda x: -x[1])[:10]:\n",
    "#         print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-papua",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "excess-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings:\n",
    "    \"\"\"\n",
    "    Instance variables:\n",
    "        - embed_dict_master - holds all qnode to embedding mappings as a dictionary\n",
    "        - embedding_list - list of all keys of the above dictionary\n",
    "    \"\"\"\n",
    "    def __init__(self, has_embeddings_include: bool = True):\n",
    "        self.embed_dict_master = {}\n",
    "        self.emb_list = ['text_7_props', 'complex', 'transe', 'abstract_first_sent']\n",
    "        if has_embeddings_include: # TODO\n",
    "            self.emb_list += ['has_h', 'has_a', 'has_s']\n",
    "        self.embedding_lengths = {}\n",
    "        \n",
    "        for emb_key in tqdm(self.emb_list, desc='Input Embeddings', leave=False):\n",
    "            self.embed_dict_master[emb_key] = self.fetch_embedding(emb_key)\n",
    "            \n",
    "        self.fetch_embedding_stats()\n",
    "        print(\"Fetched all input embeddings\")\n",
    "\n",
    "    def fetch_embedding(self, emb_key):\n",
    "        emb = Utils.deserialize_embedding_dict(\n",
    "                json.load(open(INPUT_EMB_FOLDER_PATH+emb_key+'_orig_embedding_dict.json'))\n",
    "            )\n",
    "        print(f\"OG Coverage of {emb_key}: {Utils.check_coverage(emb)}\")\n",
    "        return Utils.fill_coverage(\n",
    "                emb, emb_key\n",
    "            )\n",
    "\n",
    "    def fetch_embedding_stats(self):\n",
    "        for emb_name in self.embed_dict_master.keys():\n",
    "            self.embedding_lengths[emb_name] = len(next(iter(self.embed_dict_master[emb_name].values())))\n",
    "            print(f\"Embedding: {emb_name}, Size: {len(self.embed_dict_master[emb_name].keys())}, Length: {self.embedding_lengths[emb_name]}\")\n",
    "\n",
    "class ReducedInputEmbeddings:\n",
    "    def __init__(self, embed_dict_master, final_embed_len):\n",
    "        self.embed_dict_master = copy.deepcopy(embed_dict_master)\n",
    "        self.final_embed_len = final_embed_len\n",
    "        for key in tqdm(self.embed_dict_master.keys()):\n",
    "#             tsne = TSNE(final_embed_len, verbose=1, method='exact')\n",
    "            tfmr = PCA(final_embed_len)\n",
    "            tfmr_proj = tfmr.fit_transform(pd.DataFrame(list(self.embed_dict_master[key].values())))\n",
    "            tfmr_proj = normalize(tfmr_proj, axis=0)\n",
    "            for w_key, emb in zip(self.embed_dict_master[key].keys(), tfmr_proj):\n",
    "                self.embed_dict_master[key][w_key] = emb        \n",
    "                \n",
    "    def generate_concatenated_embedding_dict(self, key_comb):\n",
    "        embedDict = defaultdict(list)\n",
    "        masterKeySet = set()\n",
    "        for key in key_comb:\n",
    "            for qnode in self.embed_dict_master[key]:\n",
    "                masterKeySet.add(qnode)\n",
    "        for qnode in masterKeySet:\n",
    "            for key in key_comb:\n",
    "                if qnode in self.embed_dict_master[key]:\n",
    "                    embedDict[qnode] = embedDict[qnode] + (self.embed_dict_master[key][qnode].tolist())\n",
    "                else:\n",
    "#                     print(\"Hit missing elem branch for concatenation\")\n",
    "                    embedDict[qnode] = embedDict[qnode] + [0]*self.final_embed_len\n",
    "            embedDict[qnode] = np.array(embedDict[qnode])\n",
    "        return dict(embedDict)\n",
    "\n",
    "class InputScoreTables:\n",
    "    def __init__(self, embed_dict_master, exception_cols, wordsim_new=True):\n",
    "        self.input_score_tables = {}\n",
    "        self.wordsim_new = wordsim_new\n",
    "        print(f\"Fetching {'new' if wordsim_new else 'old'} wordsim score tables and eval file\")\n",
    "        if wordsim_new:\n",
    "            self.input_score_tables['classSim'] = pd.read_csv(WORDSIM_CLASS_SIM_FILE)\n",
    "            self.input_score_tables['JC'] = pd.read_csv(WORDSIM_JC_SIM_FILE)\n",
    "            self.input_score_tables['topSim'] = pd.read_csv(WORDSIM_TOP_SIM_FILE)\n",
    "        else:\n",
    "            self.input_score_tables['classSim'] = pd.read_csv(WORDSIM_OLD_CLASS_SIM_FILE)\n",
    "            self.input_score_tables['JC'] = pd.read_csv(WORDSIM_OLD_JC_SIM_FILE)\n",
    "            self.input_score_tables['topSim'] = pd.read_csv(WORDSIM_OLD_TOP_SIM_FILE)\n",
    "            \n",
    "        self.input_score_tables['classSim']['embedding_na'] = self.input_score_tables['classSim']['embedding_cos_sim'].isna()\n",
    "        self.input_score_tables['JC']['embedding_na'] = self.input_score_tables['JC']['embedding_cos_sim'].isna()\n",
    "        self.input_score_tables['topSim']['embedding_na'] = self.input_score_tables['topSim']['embedding_cos_sim'].isna()\n",
    "        \n",
    "        if embed_dict_master is not None:\n",
    "            for emb in embed_dict_master:\n",
    "#                 print(f\"Emb: {emb}\")\n",
    "                self.input_score_tables[emb] = self.construct_wsim_tab(embed_dict_master[emb])\n",
    "        self.input_score_tables['average'] = self.get_averaged_dict(exception_cols)\n",
    "    \n",
    "    def construct_wsim_tab(self, embed_dict):\n",
    "        if self.wordsim_new:\n",
    "            eval_dataset = evalD.wordsim_df.copy()\n",
    "        else:\n",
    "            eval_dataset = evalD.old_wordsim_df.copy()\n",
    "\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset.apply(lambda p: Utils.determine_cos_sim(embed_dict[p['word1_kg_id']], embed_dict[p['word2_kg_id']]) \n",
    "                                                   if p['word1_kg_id'] in embed_dict and p['word2_kg_id'] in embed_dict \n",
    "                                                   else None, axis=1)\n",
    "        eval_dataset['embedding_na'] = eval_dataset['embedding_cos_sim'].isna()\n",
    "#         print(f\"Coverage: {len(eval_dataset) - eval_dataset['embedding_cos_sim'].isna().sum()}\")\n",
    "        eval_dataset['embedding_cos_sim'].fillna(eval_dataset['embedding_cos_sim'].mean(skipna=True), inplace=True)\n",
    "        \n",
    "        # Scale abs value of cosine similarities to 1,4 strictly\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset['embedding_cos_sim'].apply(lambda p: 4 - 3 * abs(p))\n",
    "        \n",
    "        return eval_dataset\n",
    "        \n",
    "    def get_pairwise_dict(self, tab_key):\n",
    "        if tab_key not in self.input_score_tables:\n",
    "            raise \"Key not present in table\"\n",
    "        return {(row['word1_kg_id'], row['word2_kg_id']): row['embedding_cos_sim'] for _, row in self.input_score_tables[tab_key].iterrows()}\n",
    "    \n",
    "    def get_averaged_dict(self, exception_cols):\n",
    "        print(f\"Returning averaged scores from {len(set(self.input_score_tables.keys()) - exception_cols)} algorithms - {set(self.input_score_tables.keys()) - exception_cols}\")\n",
    "        final_dict = defaultdict(list)\n",
    "        for tab_key in set(self.input_score_tables.keys()) - exception_cols:\n",
    "            for _, row in self.input_score_tables[tab_key].iterrows():\n",
    "                if row['embedding_na'] == False:\n",
    "                    final_dict[(row['word1_kg_id'], row['word2_kg_id'])].append(row['embedding_cos_sim'])\n",
    "                else:\n",
    "                    print('na embedding was present, hence skipped')\n",
    "        for key in final_dict:\n",
    "            final_dict[key] = np.mean(np.array(final_dict[key]))\n",
    "        \n",
    "        if self.wordsim_new:\n",
    "            eval_dataset = evalD.wordsim_df.copy()\n",
    "        else:\n",
    "            eval_dataset = evalD.old_wordsim_df.copy()\n",
    "\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset.apply(lambda p: final_dict[(p['word1_kg_id'], p['word2_kg_id'])], axis=1)\n",
    "        \n",
    "        return eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-secretary",
   "metadata": {},
   "source": [
    "## NeighborDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "detected-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborDatasets:\n",
    "    \"\"\"\n",
    "    Instance variables:\n",
    "        - neighbors_dict_master - holds all qnode to neighbor qnode mappings as a dictionary\n",
    "        - basis_list - list of all keys of the above dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_datasets_fetch: bool = False, probase_datasets_fetch: bool = True):\n",
    "        self.neighbors_dict_master = {}\n",
    "        \n",
    "        pbar = tqdm(desc='Neighbor Datasets', leave=False, total = \n",
    "                    3\n",
    "                    + (3 if class_datasets_fetch else 0) \n",
    "                    + (1 if probase_datasets_fetch else 0) \n",
    "                   )\n",
    "        \n",
    "        bert_P279_child_par_df = pd.read_csv(P279_CHILD_PAR_DISTILBERT_COSSIM_FILE)\n",
    "#         bert_P279_child_par_df_cross_enc = pd.read_csv('../data/Master_P279_dataset/P279ChildPar_filtered_cross_enc.csv')\n",
    "        bert_P279_siblings_df = pd.read_csv(P279_SIBLINGS_DISTILBERT_COSSIM_FILE)\n",
    "#         bert_P279_siblings_df_cross_enc = pd.read_csv('../data/Master_P279_dataset/P279Siblings_transP279_filtered_min_cols_with_desc_dups_removed_cross_enc.csv')\n",
    "        \n",
    "        self.neighbors_dict_master['bert_child_par'] = self.fetch_neighbours(bert_P279_child_par_df)\n",
    "        pbar.update(1)\n",
    "        self.neighbors_dict_master['bert_siblings'] = self.fetch_neighbours(bert_P279_siblings_df)\n",
    "        pbar.update(1)\n",
    "        self.neighbors_dict_master['bert_all'] = self.fetch_neighbours(pd.concat([\n",
    "                bert_P279_child_par_df, bert_P279_siblings_df\n",
    "            ]))\n",
    "        pbar.update(1)\n",
    "        \n",
    "#         self.neighbors_dict_master['cross_enc_child_par'] = self.fetch_neighbours(bert_P279_child_par_df)\n",
    "#         pbar.update(1)\n",
    "#         self.neighbors_dict_master['cross_enc_siblings'] = self.fetch_neighbours(bert_P279_siblings_df)\n",
    "#         pbar.update(1)\n",
    "#         self.neighbors_dict_master['cross_enc_all'] = self.fetch_neighbours(pd.concat([\n",
    "#                 bert_P279_child_par_df, bert_P279_siblings_df\n",
    "#             ]))\n",
    "#         pbar.update(1)\n",
    "            \n",
    "        if class_datasets_fetch:\n",
    "            class_P279_child_par_df = pd.read_csv(P279_CHILD_PAR_CLASSSIM_FILE)\n",
    "            class_P279_child_par_df['similarity_value'] = class_P279_child_par_df['classSim']\n",
    "            \n",
    "            class_P279_siblings_df = pd.read_csv(P279_SIBLINGS_CLASSSIM_FILE)\n",
    "            class_P279_siblings_df['similarity_value'] = class_P279_siblings_df['classSim']\n",
    "            \n",
    "            self.neighbors_dict_master['class_child_par'] = self.fetch_neighbours(class_P279_child_par_df)\n",
    "            pbar.update(1)\n",
    "            self.neighbors_dict_master['class_siblings'] = self.fetch_neighbours(class_P279_siblings_df)\n",
    "            pbar.update(1)\n",
    "            self.neighbors_dict_master['class_all'] = self.fetch_neighbours(pd.concat([\n",
    "                    class_P279_child_par_df, class_P279_siblings_df\n",
    "                ]))\n",
    "            pbar.update(1)\n",
    "\n",
    "        if probase_datasets_fetch:\n",
    "            probase_df = self.process_probase(PROBASE_FINAL_FILE)\n",
    "            \n",
    "            self.neighbors_dict_master['probase'] = self.fetch_neighbours(probase_df)\n",
    "            pbar.update(1)\n",
    "#             self.neighbors_dict_master['probase+bert_all'] = self.fetch_neighbours(pd.concat([\n",
    "#                     bert_P279_child_par_df, bert_P279_siblings_df, probase_df\n",
    "#                 ]))\n",
    "#             pbar.update(1)\n",
    "        \n",
    "        self.basis_list = list(self.neighbors_dict_master.keys())\n",
    "        \n",
    "        pbar.close()\n",
    "\n",
    "        print(f\"Fetched neighbour datasets: {self.basis_list}\")\n",
    "    \n",
    "    def process_probase(self, probase_file_path):\n",
    "        probase_df = pd.read_csv(probase_file_path)\n",
    "#         probase_df = probase_df.rename(columns={'n1_final_qnode': 'node1', 'n2_final_qnode': 'node2', 'sim': 'similarity_value'})\n",
    "        probase_df['similarity_value'] = 0.5 + 0.5 * probase_df['similarity_value']\n",
    "        \n",
    "        return probase_df\n",
    "        \n",
    "    def fetch_neighbours(self, df):\n",
    "        neighbours_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            if row.node1 not in neighbours_dict:\n",
    "                neighbours_dict[row.node1] = []\n",
    "            neighbours_dict[row.node1].append((row.node2, row.similarity_value))\n",
    "\n",
    "            if row.node2 not in neighbours_dict:\n",
    "                neighbours_dict[row.node2] = []\n",
    "            neighbours_dict[row.node2].append((row.node1, row.similarity_value))\n",
    "#         print(max([len(neigh) for neigh in neighbours_dict.values()]))\n",
    "        \n",
    "        return neighbours_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-service",
   "metadata": {},
   "source": [
    "## EvaluationDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "through-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationDatasets:\n",
    "    def __init__(self):\n",
    "        self.wordsim_df = pd.read_csv(WORDSIM_DF)\n",
    "        self.wordsim_df.category = self.wordsim_df.Avg.apply(Utils.label_samples)\n",
    "        self.fetch_distribution_stats(\"Wordsim-353\", self.wordsim_df)\n",
    "        \n",
    "        self.old_wordsim_df = pd.read_csv(WORDSIM_OLD_FINAL_FILE)\n",
    "        self.wordsim_df.category = self.wordsim_df.Avg.apply(Utils.label_samples)\n",
    "        self.fetch_distribution_stats(\"Wordsim-353 OLD\", self.old_wordsim_df)\n",
    "        \n",
    "#         self.wiki_cs_df = pd.read_csv('../data/wikidata-cs_categorized.csv')\n",
    "#         self.fetch_distribution_stats(\"Wikidata CS\", self.wiki_cs_df)\n",
    "        \n",
    "#         self.concept_net_df = pd.read_csv('../data/kgtk_conceptnet_evaluation.csv')\n",
    "#         self.fetch_distribution_stats(\"Concept Net\", self.concept_net_df)\n",
    "        \n",
    "        self.get_coverage_nodes()\n",
    "        \n",
    "    def fetch_distribution_stats(self, name, dataset):\n",
    "        print(f\"Dataset: {name}\")\n",
    "        print(dataset.category.value_counts())\n",
    "    \n",
    "    def get_coverage_nodes(self):\n",
    "        self.coverage = set(\n",
    "                        self.wordsim_df['word1_kg_id'].to_list() \n",
    "                        + self.wordsim_df['word2_kg_id'].to_list())\n",
    "#                         + self.wiki_cs_df['word1_kg_id'].to_list() \n",
    "#                         + self.wiki_cs_df['word2_kg_id'].to_list()\n",
    "#                         + self.concept_net_df['word1_kg_id'].to_list()\n",
    "#                         + self.concept_net_df['word2_kg_id'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-growth",
   "metadata": {},
   "source": [
    "## ResultMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "necessary-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultMetrics:\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_classification_results(cls,\n",
    "            embed_dict, \n",
    "            eval_dataset,\n",
    "            get_output_values: bool = False,\n",
    "            old_accuracy = None\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - embed_dict - dictionary of qnodes with node embeddings as its values\n",
    "            - eval_dataset - evaluation dataset as pandas dataframe that must have the \n",
    "                following columns for this function to work correctly:\n",
    "                * word1_kg_id - Qnode of node1 in the evaluation pair\n",
    "                * word2_kg_id - Qnode of node2 in the evaluation pair\n",
    "                * category - Category of the evaluation pair. One of the labels: I/U/M\n",
    "        Outputs:\n",
    "            - response_dict - Returns a dictionary with the following keys:\n",
    "                * covered_pairs - Indicates the number of pairs of the evaluation dataset that the \n",
    "                    embedding dictionary can cover\n",
    "                \n",
    "        \"\"\"\n",
    "        response_dict = {}\n",
    "        \n",
    "        eval_dataset = eval_dataset.copy()\n",
    "\n",
    "        missing_words_set = set(\n",
    "            eval_dataset[eval_dataset.word1_kg_id.apply(lambda p: p not in embed_dict)].word1_kg_id.to_list() \n",
    "            + eval_dataset[eval_dataset.word2_kg_id.apply(lambda p: p not in embed_dict)].word2_kg_id.to_list()\n",
    "        )\n",
    "        \n",
    "        response_dict['covered_pairs'] = len(eval_dataset)\n",
    "\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset.apply(lambda p: Utils.determine_cos_sim(embed_dict[p['word1_kg_id']], embed_dict[p['word2_kg_id']]) \n",
    "                                                   if p['word1_kg_id'] in embed_dict and p['word2_kg_id'] in embed_dict \n",
    "                                                   else None, axis=1)\n",
    "        \n",
    "        eval_dataset['embedding_cos_sim'].fillna(eval_dataset['embedding_cos_sim'].mean(skipna=True), inplace=True)\n",
    "        \n",
    "        # Scale abs value of cosine similarities to 1,4 strictly\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset['embedding_cos_sim'].apply(lambda p: 4 - 3 * abs(p))\n",
    "\n",
    "        response_dict['accuracy'] = 100 * accuracy_score(\n",
    "                eval_dataset['category'],\n",
    "                eval_dataset['embedding_cos_sim'].apply(Utils.label_samples)\n",
    "            )\n",
    "    \n",
    "        response_dict['classification_report'] = classification_report(\n",
    "                eval_dataset['category'], \n",
    "                eval_dataset['embedding_cos_sim'].apply(Utils.label_samples), \n",
    "                output_dict=True\n",
    "            )\n",
    "\n",
    "        response_dict['conf_matrix'] = confusion_matrix(\n",
    "                eval_dataset['category'], \n",
    "                eval_dataset['embedding_cos_sim'].apply(Utils.label_samples), \n",
    "                labels=Utils.LABELS\n",
    "            )\n",
    "        if 'Avg' in eval_dataset.columns:\n",
    "            response_dict['KT'] = stats.kendalltau(eval_dataset['Avg'], eval_dataset['embedding_cos_sim']).correlation\n",
    "            response_dict['SR'] = stats.spearmanr(eval_dataset['Avg'], eval_dataset['embedding_cos_sim']).correlation\n",
    "            response_dict['RMSE'] = mean_squared_error(eval_dataset['Avg'], eval_dataset['embedding_cos_sim'], squared=False)\n",
    "        else:\n",
    "            response_dict['KT'] = None\n",
    "            response_dict['SR'] = None\n",
    "            response_dict['RMSE'] = None\n",
    "        \n",
    "        if old_accuracy is not None:\n",
    "            response_dict['increase_acc'] = response_dict['accuracy'] - old_accuracy\n",
    "        else:\n",
    "            response_dict['increase_acc'] = None\n",
    "        \n",
    "        if get_output_values:\n",
    "            response_dict['preds'] = eval_dataset['embedding_cos_sim'].apply(Utils.label_samples)\n",
    "\n",
    "        return response_dict, \\\n",
    "                (response_dict['covered_pairs'],  \\\n",
    "                 response_dict['accuracy'], \\\n",
    "                 response_dict['increase_acc'], \\\n",
    "                 \n",
    "                 response_dict['classification_report']['I']['precision'],  \\\n",
    "                 response_dict['classification_report']['I']['recall'],  \\\n",
    "                 response_dict['classification_report']['I']['f1-score'],   \\\n",
    "                 \n",
    "                 response_dict['classification_report']['M']['precision'],  \\\n",
    "                 response_dict['classification_report']['M']['recall'],  \\\n",
    "                 response_dict['classification_report']['M']['f1-score'], \\\n",
    "                 \n",
    "                 response_dict['classification_report']['U']['precision'],  \\\n",
    "                 response_dict['classification_report']['U']['recall'],  \\\n",
    "                 response_dict['classification_report']['U']['f1-score'], \\\n",
    "                \n",
    "                 response_dict['KT'], \\\n",
    "                 response_dict['SR'],\n",
    "                 response_dict['RMSE'])\n",
    "    \n",
    "    @classmethod\n",
    "    def fetch_best_result_for_emb(cls, results_df, emb_col, target_col, iter_col, highest: bool = True):\n",
    "        opt_value = {}\n",
    "        for _, row in results_df.iterrows():\n",
    "            if row[emb_col] not in opt_value:\n",
    "                opt_value[row[emb_col]] = {'opt_metric': float('-inf') if highest else float('inf'),\n",
    "                                     'opt_row': [], 'old_row': []}\n",
    "            if row[iter_col] == 0:\n",
    "                opt_value[row[emb_col]]['old_row'] = row\n",
    "            else:\n",
    "                if (highest and row[target_col] > opt_value[row[emb_col]]['opt_metric']) \\\n",
    "                        or (not(highest) and row[target_col] < opt_value[row[emb_col]]['opt_metric']):\n",
    "                    opt_value[row[emb_col]]['opt_metric'] = row[target_col]\n",
    "                    opt_value[row[emb_col]]['opt_row'] = row\n",
    "        best_results = []\n",
    "        for emb_key in opt_value:\n",
    "            best_results.append(opt_value[emb_key]['old_row'])\n",
    "            best_results.append(opt_value[emb_key]['opt_row'])\n",
    "        return pd.DataFrame(best_results, columns = results_df.columns)\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_classification_n_regression_stats(cls, ist, suffix, standard_labels=True):\n",
    "        q_size = len(evalD.wordsim_df) // 4\n",
    "        if not(standard_labels):\n",
    "            print(f\"At most {q_size} rows in each quartile\")\n",
    "            temp_wordsim_df = evalD.wordsim_df.sort_values(by=['Avg', 'word1_kg_id', 'word2_kg_id'])\n",
    "            quantile_sets = {'Q'+str(i+1): set(temp_wordsim_df[q_size*i:q_size*(i+1)].apply(lambda p: (p.word1_kg_id, p.word2_kg_id), axis=1).to_list()) \n",
    "                                 for i in range(4) }\n",
    "#             quantiles = evalD.wordsim_df.Avg.quantile([0, 0.25, 0.5, 0.75, 1]).to_list()\n",
    "#             quantiles[0], quantiles[-1] = float('-inf'), float('inf')\n",
    "#             print(f\"Quantiles being used: {quantile_sets}\")\n",
    "            wordsim_cats = evalD.wordsim_df.apply(Utils.alt2_label_samples, args=(quantile_sets,), axis=1)\n",
    "        else:\n",
    "            wordsim_cats = evalD.wordsim_df.category\n",
    "        \n",
    "        eval_df = evalD.wordsim_df.copy()\n",
    "            \n",
    "        results = []\n",
    "        for tab_key in ist.input_score_tables:\n",
    "            cosSimPreds_df = ist.input_score_tables[tab_key]\n",
    "            response_dict = {}\n",
    "            if standard_labels:\n",
    "                preds = cosSimPreds_df['embedding_cos_sim'].apply(Utils.label_samples)\n",
    "            else:\n",
    "                temp_wordsim_df = cosSimPreds_df.sort_values(by=['embedding_cos_sim', 'word1_kg_id', 'word2_kg_id'])\n",
    "                quantile_sets = {'Q'+str(i+1): set(temp_wordsim_df[q_size*i:q_size*(i+1)].apply(lambda p: (p.word1_kg_id, p.word2_kg_id), axis=1).to_list()) \n",
    "                                 for i in range(4) }\n",
    "                preds = cosSimPreds_df.apply(Utils.alt2_label_samples, args=(quantile_sets,), axis=1)\n",
    "            eval_df[tab_key] = cosSimPreds_df['embedding_cos_sim']\n",
    "            eval_df[tab_key+'_cat'] = preds\n",
    "            response_dict['accuracy'] = 100 * accuracy_score(\n",
    "                    wordsim_cats,\n",
    "                    preds\n",
    "                )\n",
    "\n",
    "            response_dict['classification_report'] = classification_report(\n",
    "                    wordsim_cats,\n",
    "                    preds, \n",
    "                    output_dict=True\n",
    "                )\n",
    "\n",
    "            response_dict['KT'] = stats.kendalltau(cosSimPreds_df['Avg'], cosSimPreds_df['embedding_cos_sim']).correlation\n",
    "            response_dict['SR'] = stats.spearmanr(cosSimPreds_df['Avg'], cosSimPreds_df['embedding_cos_sim']).correlation\n",
    "            response_dict['RMSE'] = mean_squared_error(cosSimPreds_df['Avg'], cosSimPreds_df['embedding_cos_sim'], squared=False)\n",
    "            \n",
    "            # SVR related stats\n",
    "            temp_dict = {'basis': '', 'emb': tab_key, 'weightedness': True, \n",
    "                'iter_num': 0, 'weight_case': None, 'svm_input': 'score'}\n",
    "            svr_res = SVMProcedures.execute_supervised_scenario(\n",
    "                evalD.wordsim_df, temp_dict, ist.get_pairwise_dict(tab_key), \n",
    "                {},num_of_splits=10,\n",
    "                comb_mode=False, SVC_or_SVR='SVR', score_table_mode=True\n",
    "            )\n",
    "            \n",
    "            if standard_labels:\n",
    "                results.append([tab_key, response_dict['accuracy'], response_dict['classification_report']['macro avg']['precision'], \\\n",
    "                            response_dict['classification_report']['macro avg']['recall'], \\\n",
    "                            response_dict['classification_report']['macro avg']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['I']['precision'], \\\n",
    "                            response_dict['classification_report']['I']['recall'], \\\n",
    "                            response_dict['classification_report']['I']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['M']['precision'], \\\n",
    "                            response_dict['classification_report']['M']['recall'], \\\n",
    "                            response_dict['classification_report']['M']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['U']['precision'], \\\n",
    "                            response_dict['classification_report']['U']['recall'], \\\n",
    "                            response_dict['classification_report']['U']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['KT'], response_dict['SR'], response_dict['RMSE'], svr_res[-2], svr_res[-1], svr_res[-3]])\n",
    "            else:\n",
    "                results.append([tab_key, response_dict['accuracy'], response_dict['classification_report']['macro avg']['precision'], \\\n",
    "                            response_dict['classification_report']['macro avg']['recall'], \\\n",
    "                            response_dict['classification_report']['macro avg']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['Q1']['precision'], \\\n",
    "                            response_dict['classification_report']['Q1']['recall'], \\\n",
    "                            response_dict['classification_report']['Q1']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['Q2']['precision'], \\\n",
    "                            response_dict['classification_report']['Q2']['recall'], \\\n",
    "                            response_dict['classification_report']['Q2']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['Q3']['precision'], \\\n",
    "                            response_dict['classification_report']['Q3']['recall'], \\\n",
    "                            response_dict['classification_report']['Q3']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['Q4']['precision'], \\\n",
    "                            response_dict['classification_report']['Q4']['recall'], \\\n",
    "                            response_dict['classification_report']['Q4']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['KT'], response_dict['SR'], response_dict['RMSE'], svr_res[-2], svr_res[-1], svr_res[-3]])\n",
    "        if standard_labels:\n",
    "            res_df = pd.DataFrame(results, columns=['algorithm', 'accuracy', 'P', 'R', 'F1', 'I P', 'I R', 'I F1', 'M P', 'M R', 'M F1', 'U P', 'U R', 'U F1', 'Kendall Tau', 'Spearman Rank', 'RMSE', 'SVR Kendall Tau', 'SVR Spearman Rank', 'SVR RMSE'])\n",
    "        else:\n",
    "            res_df = pd.DataFrame(results, columns=['algorithm', 'accuracy', 'P', 'R', 'F1', 'Q1 P', 'Q1 R', 'Q1 F1', 'Q2 P', 'Q2 R', 'Q2 F1', 'Q3 P', 'Q3 R', 'Q3 F1', 'Q4 P', 'Q4 R', 'Q4 F1', 'Kendall Tau', 'Spearman Rank', 'RMSE', 'SVR Kendall Tau', 'SVR Spearman Rank', 'SVR RMSE'])\n",
    "        res_df.to_csv('../data/retrofitting/score_table_algorithms_results.' + suffix + '.' + Utils.today_date + '.csv', index=False)\n",
    "        return res_df, eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-cosmetic",
   "metadata": {},
   "source": [
    "## RetrofittingProcedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "prescription-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrofittingProcedures:\n",
    "    \n",
    "    np_label_samples = np.vectorize(Utils.label_samples)\n",
    "    \n",
    "    @classmethod\n",
    "    def retrofit(cls,embed_dict, neighbors_dict, weight_case, weight_assignment=False):\n",
    "        new_embed_dict = {}\n",
    "        for word in embed_dict.keys():\n",
    "            if word in neighbors_dict:\n",
    "                neighbs = neighbors_dict[word]\n",
    "                neighbs = list(filter(lambda p: p[0] in embed_dict, neighbs))\n",
    "                if len(neighbs) == 0:\n",
    "                    new_embed_dict[word] = embed_dict[word]\n",
    "                    continue\n",
    "                if weight_assignment:\n",
    "                    sum_of_sims = sum([neighb[1] for neighb in neighbs])\n",
    "                    sum_of_embs = sum([embed_dict[neighb[0]] * float(neighb[1]) for neighb in neighbs])\n",
    "                else:\n",
    "                    sum_of_sims = sum([1 for neighb in neighbs])\n",
    "                    sum_of_embs = sum([embed_dict[neighb[0]] for neighb in neighbs])\n",
    "\n",
    "                if weight_case == 1:\n",
    "                    new_embed_dict[word] = (embed_dict[word] * (len(neighbs)) + sum_of_embs) / ((len(neighbs)) + sum_of_sims)\n",
    "                elif weight_case == 2:\n",
    "                    new_embed_dict[word] = (embed_dict[word] * (len(neighbs))**2 + sum_of_embs) / ((len(neighbs))**2 + sum_of_sims)\n",
    "                elif weight_case == 0.5:\n",
    "                    new_embed_dict[word] = (embed_dict[word] * (len(neighbs))**0.5 + sum_of_embs) / ((len(neighbs))**0.5 + sum_of_sims)\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                new_embed_dict[word] = embed_dict[word]\n",
    "        return new_embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def execute_all_unsupervised_scenarios(cls,\n",
    "                emb_list, basis_list, \n",
    "                embed_dict_master, neigh_dict_master,\n",
    "                eval_dataset,\n",
    "                scenario_name: str,\n",
    "                num_of_iterations: int = 2, \n",
    "                weightedness_list: list = [True],\n",
    "                weight_cases_list: list = [1],\n",
    "                get_output_values: bool = False\n",
    "            ):\n",
    "        \n",
    "        new_embed_dict_master = {}\n",
    "        responses_dict_master = {}\n",
    "        results = []\n",
    "        \n",
    "        for basis in tqdm(basis_list, desc='Basis', leave=False):\n",
    "            for emb in tqdm(emb_list, desc='Embedding', leave=False):\n",
    "                for weightedness in weightedness_list:\n",
    "                    for weight_case in tqdm(weight_cases_list, desc='Weight Case', leave=False):\n",
    "                        # Base Reference Initializations and Calculations\n",
    "                        embed_dict = embed_dict_master[emb]\n",
    "                        responses_dict, result_values = ResultMetrics.compute_classification_results(\n",
    "                            embed_dict, eval_dataset, get_output_values=get_output_values, old_accuracy=None)\n",
    "                        results.append([emb, basis, weight_case, weightedness, 0, 'base', *result_values, 0])\n",
    "                        old_accuracy = responses_dict['accuracy']\n",
    "                        \n",
    "                        for iter_num in tqdm(range(1,num_of_iterations+1), desc='Iteration', leave=False):\n",
    "                            start_time = time()\n",
    "                            \n",
    "                            case_name = emb + '_' + basis + '_' + str(weight_case) + ('_weighted' if weightedness else '_unweighted')\n",
    "                            \n",
    "                            new_embed_dict = cls.retrofit(embed_dict, neigh_dict_master[basis], weight_case, weightedness)\n",
    "                            \n",
    "                            responses_dict, result_values = ResultMetrics.compute_classification_results(\n",
    "                                new_embed_dict, eval_dataset, get_output_values=get_output_values, old_accuracy=old_accuracy)\n",
    "                            \n",
    "                            results.append([emb, basis, weight_case, weightedness, iter_num, case_name, \\\n",
    "                                                *result_values, \\\n",
    "                                                time() - start_time\n",
    "                                            ])\n",
    "                            \n",
    "                            new_embed_dict_master[case_name] = embed_dict = new_embed_dict\n",
    "                            responses_dict_master[case_name] = responses_dict\n",
    "\n",
    "        #                     if iter_num == num_of_iterations and highestOne:\n",
    "        #                         case_name = gR[0] + '_' + gR[1] + '_' + str(gR[2]) + '_weighted'\n",
    "        #                         new_embed_dict_master[case_name] = serializeEmbeddingDict(new_embed_dict_master[case_name])\n",
    "        #                         highestOne = False\n",
    "        #                         json.dump(new_embed_dict_master[case_name],open('../data/Master_P279_dataset/embeddings/new_embedding_dict_'+case_name+'.json','w'))\n",
    "        #                         new_embed_dict_master[case_name] = deserializeEmbeddingDict(new_embed_dict_master[case_name])\n",
    "#         print(results)\n",
    "#         ['text_7_props', 'bert_child_par', 1, True, 0, 'base', 344, 56.68604651162791, None, 0.30952380952380953, \n",
    "#          0.65, 0.41935483870967744, 0.6289752650176679, 0.8054298642533937, 0.7063492063492064, \n",
    "#          0.21052631578947367, 0.038834951456310676, 0.06557377049180328, 0.31127513538205615, 0.4132700578622946]\n",
    "        resultsDF = pd.DataFrame(results, columns=['Embedding', 'Basis', 'Weight Case', 'Weightedness', \n",
    "                                                   'Iteration Num', 'Case Name', \\\n",
    "                                                   'No. of Pairs Covered', 'Accuracy', 'Increase in Accuracy', \\\n",
    "                                                   'I Precision', 'I Recall', 'I F1-Score', \\\n",
    "                                                   'M Precision', 'M Recall', 'M F1-Score', \\\n",
    "                                                   'U Precision', 'U Recall', 'U F1-Score',\n",
    "                                                   'KT Correlation', 'SpearmanR Correlation', 'RMSE', \\\n",
    "                                                   'Time to Retrofit'])\n",
    "        resultsDF.to_csv('../data/retrofitting/retro_unsup_results.' + scenario_name + '.'+ Utils.today_date +'.csv', index=False)\n",
    "#         best_results_df = ResultMetrics.fetch_best_result_for_emb(resultsDF, 'Embedding', 'Accuracy', 'Iteration Num', highest=True)\n",
    "#         best_results_df.to_csv('../data/retrofitting/retro_unsup_results.' + scenario_name + '.'+ Utils.today_date +'.best.csv', index=False)\n",
    "        \n",
    "#         cls.save_needed_embeddings(new_embed_dict_master)\n",
    "        \n",
    "        return new_embed_dict_master, responses_dict_master\n",
    "    \n",
    "    @classmethod\n",
    "    def save_all_embeddings(cls, new_embed_dict_master):\n",
    "        for case_name in new_embed_dict_master:\n",
    "            json.dump(Utils.serialize_embedding_dict(new_embed_dict_master[case_name]), open(INPUT_EMB_FOLDER_PATH + 'new_embeddings/' + case_name + '.' + Utils.today_date + '.json', 'w'))\n",
    "    \n",
    "    @classmethod\n",
    "    def save_needed_embeddings(cls, new_embed_dict_master):\n",
    "        for case_name in new_embed_dict_master:\n",
    "            temp = {key: new_embed_dict_master[case_name][key] for key in new_embed_dict_master[case_name] if key in evalD.coverage}\n",
    "            new_embed_dict_master[case_name] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-implementation",
   "metadata": {},
   "source": [
    "## SVMProcedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "missing-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMProcedures:\n",
    "    @classmethod\n",
    "    def execute_supervised_scenario(cls,\n",
    "                eval_dataset, case, embed_dict_master, new_embed_dict_master, \n",
    "                num_of_splits = 10, \n",
    "                comb_mode: bool = False, SVC_or_SVR: str = 'SVC', \n",
    "                score_table_mode: bool = False\n",
    "            ):\n",
    "        \n",
    "        X = []        \n",
    "        \n",
    "        ################\n",
    "        # 2 Approaches based on argument: `comb_mode`\n",
    "        ################\n",
    "        \n",
    "        if comb_mode: ########## COMBINATION MODE CODE ####################\n",
    "            case_name = \" & \".join(case['emb']) + '_' + case['basis'] + '_' + str(case['weight_case']) + ('_weighted' if case['weightedness'] else '_unweighted')\n",
    "            \n",
    "            for _, row in eval_dataset.iterrows():\n",
    "                if case['svm_input'] == 'emb':\n",
    "                    tempX = []\n",
    "                    for individual_emb in case['emb']:\n",
    "                        ind_case_name = individual_emb + '_' + case['basis'] + '_' + str(case['weight_case']) + ('_weighted' if case['weightedness'] else '_unweighted')\n",
    "                        if case['iter_num'] != 0 and ind_case_name not in new_embed_dict_master:\n",
    "                            return case_name, case, None\n",
    "                        if case['iter_num'] == 0:\n",
    "                            if score_table_mode:\n",
    "                                raise \"Not yet implemented\"\n",
    "                            tempX += embed_dict_master[individual_emb][row['word1_kg_id']].tolist() + embedDictMaster[individual_emb][row['word2_kg_id']].tolist()\n",
    "                        else:\n",
    "                            tempX += new_embed_dict_master[ind_case_name][row['word1_kg_id']].tolist() + newEmbedDictMaster[ind_case_name][row['word2_kg_id']].tolist()\n",
    "                    X.append(tempX)\n",
    "                else:\n",
    "                    tempX = []\n",
    "                    for individual_emb in case['emb']:\n",
    "                        ind_case_name = individual_emb + '_' + case['basis'] + '_' + str(case['weight_case']) + ('_weighted' if case['weightedness'] else '_unweighted')\n",
    "                        if case['iter_num'] != 0 and ind_case_name not in new_embed_dict_master:\n",
    "                            return case_name, case, None\n",
    "                        if case['iter_num'] == 0:\n",
    "                            if score_table_mode:\n",
    "                                raise \"Not yet implemented\"\n",
    "                            tempX.append(abs(Utils.determine_cos_sim(\n",
    "                                embed_dict_master[individual_emb][row['word1_kg_id']], \n",
    "                                embed_dict_master[individual_emb][row['word2_kg_id']]\n",
    "                            )))\n",
    "                        else:\n",
    "                            tempX.append(abs(Utils.determine_cos_sim(\n",
    "                                new_embed_dict_master[ind_case_name][row['word1_kg_id']],\n",
    "                                new_embed_dict_master[ind_case_name][row['word2_kg_id']]\n",
    "                            )))\n",
    "                    X.append(tempX)\n",
    "\n",
    "        else: ########## NON-COMBINATION MODE CODE ####################\n",
    "            case_name = case['emb'] + '_' + case['basis'] + '_' + str(case['weight_case']) + ('_weighted' if case['weightedness'] else '_unweighted')\n",
    "            if case['iter_num'] != 0 and case_name not in new_embed_dict_master:\n",
    "                return case_name, case, None\n",
    "            for _, row in eval_dataset.iterrows():\n",
    "                if case['svm_input'] == 'emb':\n",
    "                    if case['iter_num'] == 0:\n",
    "                        if score_table_mode:\n",
    "                            raise \"Not yet implemented\"\n",
    "                        X.append(embed_dict_master[case['emb']][row['word1_kg_id']].tolist() + embed_dict_master[case['emb']][row['word2_kg_id']].tolist())\n",
    "                    else:\n",
    "                        X.append(new_embed_dict_master[case_name][row['word1_kg_id']].tolist() + new_embed_dict_master[case_name][row['word2_kg_id']].tolist())\n",
    "                else:\n",
    "                    if case['iter_num'] == 0:\n",
    "                        if score_table_mode:\n",
    "                            X.append(embed_dict_master[(row['word1_kg_id'], row['word2_kg_id'])])\n",
    "                        else:\n",
    "                            X.append(abs(Utils.determine_cos_sim(\n",
    "                                    embed_dict_master[case['emb']][row['word1_kg_id']], \n",
    "                                    embed_dict_master[case['emb']][row['word2_kg_id']]\n",
    "                                )))\n",
    "                    else:\n",
    "                        X.append(abs(Utils.determine_cos_sim(\n",
    "                                new_embed_dict_master[case_name][row['word1_kg_id']],\n",
    "                                new_embed_dict_master[case_name][row['word2_kg_id']]\n",
    "                            )))\n",
    "                    \n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        ################\n",
    "        # 2 Approaches based on argument: `SVC_or_SVR`\n",
    "        ################\n",
    "        \n",
    "        # Target split depending on SVC or SVM\n",
    "        if SVC_or_SVR == 'SVC':\n",
    "            Y = eval_dataset['category']\n",
    "        elif SVC_or_SVR == 'SVR':\n",
    "            if 'Avg' not in eval_dataset.columns:\n",
    "                raise ValueError(\"Avg column not present in the provided eval_dataset\")\n",
    "            Y = (eval_dataset['Avg'] - 1) / 3\n",
    "        else:\n",
    "            raise ValueError(\"Invalid SVC_or_SVR provided\")\n",
    "        \n",
    "        if SVC_or_SVR == 'SVC':\n",
    "            skf = StratifiedKFold(n_splits=num_of_splits, random_state=19, shuffle=True)\n",
    "            X_train_splits, X_test_splits, Y_train_splits, Y_test_splits = [], [], [], []\n",
    "            for train_index, test_index in skf.split(X, Y):\n",
    "                X_train_splits.append(X.iloc[train_index])\n",
    "                X_test_splits.append(X.iloc[test_index])\n",
    "                Y_train_splits.append(Y.iloc[train_index])\n",
    "                Y_test_splits.append(Y.iloc[test_index])\n",
    "        elif SVC_or_SVR == 'SVR':\n",
    "            skf = KFold(n_splits=num_of_splits, random_state=19, shuffle=True)\n",
    "            X_train_splits, X_test_splits, Y_train_splits, Y_test_splits = [], [], [], []\n",
    "            for train_index, test_index in skf.split(X, Y):\n",
    "                X_train_splits.append(X.iloc[train_index])\n",
    "                X_test_splits.append(X.iloc[test_index])\n",
    "                Y_train_splits.append(Y.iloc[train_index])\n",
    "                Y_test_splits.append(Y.iloc[test_index])\n",
    "\n",
    "        preds = []\n",
    "        \n",
    "        # Classifier/Regressor training depending on SVC or SVM\n",
    "        if SVC_or_SVR == 'SVC':\n",
    "            for X_train1, Y_train1, X_test1, Y_test1 in zip(X_train_splits, Y_train_splits, X_test_splits, Y_test_splits):\n",
    "                clf = make_pipeline(StandardScaler(), SVC(gamma='auto', random_state=100, max_iter=100))\n",
    "                clf.fit(X_train1, Y_train1)\n",
    "                preds.append(clf.predict(X_test1))\n",
    "                \n",
    "            acc = 0\n",
    "            for pred, Y_test1 in zip(preds, Y_test_splits):\n",
    "                acc += accuracy_score(pred, Y_test1)\n",
    "\n",
    "            return case_name, *list(case.values()), acc/num_of_splits\n",
    "        \n",
    "        elif SVC_or_SVR == 'SVR':\n",
    "            for X_train1, Y_train1, X_test1, Y_test1 in zip(X_train_splits, Y_train_splits, X_test_splits, Y_test_splits):\n",
    "                clf = make_pipeline(StandardScaler(), SVR(gamma='auto', max_iter=100))\n",
    "                clf.fit(X_train1, Y_train1)\n",
    "                preds.append(clf.predict(X_test1))\n",
    "            \n",
    "            acc = 0\n",
    "            ktCorr = 0\n",
    "            spearmanR = 0\n",
    "            for pred, Y_test1 in zip(preds, Y_test_splits):\n",
    "                acc += mean_squared_error(pred * 3 + 1, Y_test1 * 3 + 1, squared=False)\n",
    "                ktCorr += stats.kendalltau(Y_test1 * 3 + 1, pred * 3 + 1).correlation\n",
    "                spearmanR += stats.spearmanr(Y_test1 * 3 + 1, pred * 3 + 1).correlation\n",
    "                \n",
    "            return case_name, *list(case.values()), acc/num_of_splits, ktCorr/num_of_splits, spearmanR/num_of_splits\n",
    "    \n",
    "    @classmethod\n",
    "    def execute_all_supervised_scenarios(cls,\n",
    "                emb_list, basis_list, embed_dict_master, new_embed_dict_master, \n",
    "                eval_dataset, \n",
    "                scenario_name: str,\n",
    "                num_of_splits = 10,\n",
    "                comb_mode: bool = False, SVC_or_SVR: str = 'SVC', \n",
    "                num_of_iterations = 2,\n",
    "                num_of_jobs = 30\n",
    "            ):\n",
    "        \n",
    "        if not(comb_mode):\n",
    "            svm_cases_list = []\n",
    "            for basis in basis_list:\n",
    "                for emb in emb_list:\n",
    "                    for weightedness in [True]:\n",
    "                        for iter_num in range(0,num_of_iterations+1):\n",
    "                            for weight_case in [1]:\n",
    "                                for svm_input in ['score']:\n",
    "                                    temp_dict = {'basis': basis, 'emb': emb, 'weightedness': weightedness, \n",
    "                                                'iter_num': iter_num, 'weight_case': weight_case, 'svm_input': svm_input}\n",
    "                                    svm_cases_list.append(temp_dict) \n",
    "        else:\n",
    "            svm_cases_list = []\n",
    "            for basis in basis_list:\n",
    "                for emb in emb_list:\n",
    "                    for weightedness in [True]:\n",
    "                        for iter_num in range(0,num_of_iterations+1):\n",
    "                            for weight_case in [1]:\n",
    "                                for svm_input in ['score']:\n",
    "                                    for i in range(1,len(emb_list)+1):\n",
    "                                        for emb_comb in combinations(emb_list, i):\n",
    "                                            temp_dict = {'basis': basis, 'emb': emb_comb, 'weightedness': weightedness, \n",
    "                                                        'iter_num': iter_num, 'weight_case': weight_case, 'svm_input': svm_input}\n",
    "                                            svm_cases_list.append(temp_dict) \n",
    "\n",
    "        results = Parallel(n_jobs=num_of_jobs)(delayed(cls.execute_supervised_scenario)(\n",
    "                eval_dataset, caseDict, embed_dict_master, \n",
    "                new_embed_dict_master,num_of_splits,\n",
    "                comb_mode, SVC_or_SVR\n",
    "            ) for caseDict in tqdm(svm_cases_list))\n",
    "        \n",
    "        if SVC_or_SVR == 'SVC':\n",
    "            results_df = pd.DataFrame(results, columns=['Case Name','Basis','Embedding','Weightedness', 'Iteration Num', 'Weight Case', 'Technique','Accuracy'])\n",
    "#             best_results_df = ResultMetrics.fetch_best_result_for_emb(results_df, 'Embedding', 'Accuracy', 'Iteration Num', highest=True)\n",
    "#             best_results_df.to_csv('../data/retrofitting/retro_SVC_results.' + scenario_name + '.'+ Utils.today_date +'.best.csv', index=False)\n",
    "        else:\n",
    "            results_df = pd.DataFrame(results, columns=['Case Name','Basis','Embedding','Weightedness', 'Iteration Num', 'Weight Case', 'Technique','MSE', 'KT Correlation', 'SR Correlation'])\n",
    "#             best_results_df = ResultMetrics.fetch_best_result_for_emb(results_df, 'Embedding', 'MSE', 'Iteration Num', highest=False)\n",
    "#             best_results_df.to_csv('../data/retrofitting/retro_SVR_results.' + scenario_name + '.'+ Utils.today_date +'.best.csv', index=False)\n",
    "            \n",
    "        results_df.to_csv('../data/retrofitting/retro_SVM_results.' + scenario_name + '.'+ Utils.today_date +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-voluntary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "retained-frederick",
   "metadata": {},
   "source": [
    "# The Master Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "south-strand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Input Embeddings:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Coverage of text_7_props: 326\n",
      "Added 11 corrections to text_7_props\n",
      "OG Coverage of complex: 344\n",
      "Added 0 corrections to complex\n",
      "OG Coverage of transe: 344\n",
      "Added 0 corrections to transe\n",
      "OG Coverage of abstract_first_sent: 344\n",
      "Added 0 corrections to abstract_first_sent\n",
      "OG Coverage of has_h: 337\n",
      "Added 7 corrections to has_h\n",
      "OG Coverage of has_a: 20\n",
      "Added 372 corrections to has_a\n",
      "OG Coverage of has_s: 234\n",
      "Added 93 corrections to has_s\n",
      "Embedding: text_7_props, Size: 220011, Length: 1024\n",
      "Embedding: complex, Size: 241694, Length: 100\n",
      "Embedding: transe, Size: 241694, Length: 100\n",
      "Embedding: abstract_first_sent, Size: 241694, Length: 768\n",
      "Embedding: has_h, Size: 166208, Length: 200\n",
      "Embedding: has_a, Size: 28247, Length: 200\n",
      "Embedding: has_s, Size: 117083, Length: 200\n",
      "Fetched all input embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Neighbor Datasets:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched neighbour datasets: ['bert_child_par', 'bert_siblings', 'bert_all', 'probase']\n",
      "Dataset: Wordsim-353\n",
      "M    221\n",
      "U    103\n",
      "I     20\n",
      "Name: category, dtype: int64\n",
      "Dataset: Wordsim-353 OLD\n",
      "M    280\n",
      "U     44\n",
      "I     25\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "new_embed_dict_master, responses_dict_master = {}, {}\n",
    "\n",
    "# # Load all supporting files\n",
    "inp = InputEmbeddings()\n",
    "basis = NeighborDatasets()\n",
    "evalD = EvaluationDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "prepared-browse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04927e5af4714f1d8e6db6b51130f6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp_tsne = ReducedInputEmbeddings(inp.embed_dict_master, 100)\n",
    "conc_emb_dict = inp_tsne.generate_concatenated_embedding_dict(list(inp.embed_dict_master.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "circular-monte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covered_pairs': 349,\n",
       " 'accuracy': 44.69914040114613,\n",
       " 'classification_report': {'I': {'precision': 0.8888888888888888,\n",
       "   'recall': 0.32,\n",
       "   'f1-score': 0.47058823529411764,\n",
       "   'support': 25},\n",
       "  'M': {'precision': 0.8320610687022901,\n",
       "   'recall': 0.3892857142857143,\n",
       "   'f1-score': 0.5304136253041363,\n",
       "   'support': 280},\n",
       "  'U': {'precision': 0.18660287081339713,\n",
       "   'recall': 0.8863636363636364,\n",
       "   'f1-score': 0.30830039525691694,\n",
       "   'support': 44},\n",
       "  'accuracy': 0.4469914040114613,\n",
       "  'macro avg': {'precision': 0.6358509428015253,\n",
       "   'recall': 0.531883116883117,\n",
       "   'f1-score': 0.436434085285057,\n",
       "   'support': 349},\n",
       "  'weighted avg': {'precision': 0.7547560108156244,\n",
       "   'recall': 0.4469914040114613,\n",
       "   'f1-score': 0.49812532481035937,\n",
       "   'support': 349}},\n",
       " 'conf_matrix': array([[  8,  17,   0],\n",
       "        [  1, 109, 170],\n",
       "        [  0,   5,  39]]),\n",
       " 'KT': 0.4188226957013864,\n",
       " 'SR': 0.5855565621170609,\n",
       " 'RMSE': 0.8370554555481824,\n",
       " 'increase_acc': None}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_dict, _ = ResultMetrics.compute_classification_results(\n",
    "                            conc_emb_dict, evalD.old_wordsim_df, get_output_values=False, old_accuracy=None)\n",
    "responses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "freelance-daughter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covered_pairs': 344,\n",
       " 'accuracy': 55.81395348837209,\n",
       " 'classification_report': {'I': {'precision': 1.0,\n",
       "   'recall': 0.45,\n",
       "   'f1-score': 0.6206896551724138,\n",
       "   'support': 20},\n",
       "  'M': {'precision': 0.7777777777777778,\n",
       "   'recall': 0.4434389140271493,\n",
       "   'f1-score': 0.5648414985590777,\n",
       "   'support': 221},\n",
       "  'U': {'precision': 0.40669856459330145,\n",
       "   'recall': 0.8252427184466019,\n",
       "   'f1-score': 0.5448717948717948,\n",
       "   'support': 103},\n",
       "  'accuracy': 0.5581395348837209,\n",
       "  'macro avg': {'precision': 0.7281587807903597,\n",
       "   'recall': 0.5728938774912504,\n",
       "   'f1-score': 0.5768009828677622,\n",
       "   'support': 344},\n",
       "  'weighted avg': {'precision': 0.6795896541918574,\n",
       "   'recall': 0.5581395348837209,\n",
       "   'f1-score': 0.5621091835953469,\n",
       "   'support': 344}},\n",
       " 'conf_matrix': array([[  9,  10,   1],\n",
       "        [  0,  98, 123],\n",
       "        [  0,  18,  85]]),\n",
       " 'KT': 0.42603956544144383,\n",
       " 'SR': 0.5644769083815114,\n",
       " 'RMSE': 0.6932338182205002,\n",
       " 'increase_acc': None}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_dict, _ = ResultMetrics.compute_classification_results(\n",
    "                            conc_emb_dict, evalD.wordsim_df, get_output_values=False, old_accuracy=None)\n",
    "responses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "important-pollution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching new wordsim score tables and eval file\n",
      "Returning averaged scores from 9 algorithms - {'transe', 'JC', 'complex', 'topSim', 'abstract_first_sent', 'has_s', 'has_h', 'text_7_props', 'classSim'}\n"
     ]
    }
   ],
   "source": [
    "ist = InputScoreTables(inp.embed_dict_master, set(['has_a']))\n",
    "_,res_df = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim_quantiles', standard_labels=False)\n",
    "res_df.to_csv('../data/retrofitting/wordsim_quantile_analysis.'+ Utils.today_date +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "painful-mineral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['classSim', 'JC', 'topSim', 'text_7_props', 'complex', 'transe', 'abstract_first_sent', 'has_h', 'has_a', 'has_s', 'average'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ist.input_score_tables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cardiac-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching new wordsim score tables and eval file\n",
      "Returning averaged scores from 9 algorithms - {'transe', 'JC', 'complex', 'topSim', 'abstract_first_sent', 'has_s', 'has_h', 'text_7_props', 'classSim'}\n"
     ]
    }
   ],
   "source": [
    "ist = InputScoreTables(inp.embed_dict_master, set(['has_a']))\n",
    "_,res_df = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim_quantiles', standard_labels=True)\n",
    "res_df.to_csv('../data/retrofitting/wordsim_all_algo_scores.'+ Utils.today_date +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "neither-ambassador",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d9c25492324e0295b3c44dbcd8dc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_master_time = time()\n",
    "\n",
    "\n",
    "# Wordsim executions\n",
    "# new_embed_dict_master['wordsim'], responses_dict_master['wordsim'] = RetrofittingProcedures.execute_all_unsupervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, basis.neighbors_dict_master, \n",
    "#                                                                             evalD.wordsim_df, \"wordsim_ind\")\n",
    "\n",
    "# # ist = InputScoreTables(inp.embed_dict_master)\n",
    "# # _ = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim')\n",
    "\n",
    "# # ist = InputScoreTables(inp.embed_dict_master, False)\n",
    "# # _ = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim_old')\n",
    "\n",
    "# print(\"Analysed wordsim_ind\")\n",
    "\n",
    "# SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, \n",
    "#                                             new_embed_dict_master['wordsim'], \n",
    "#                                             evalD.wordsim_df, \"SVC_Wordsim\",\n",
    "#                                             comb_mode = False, SVC_or_SVR = 'SVC')\n",
    "# print(\"Analysed SVC_Wordsim\")\n",
    "\n",
    "SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, \n",
    "                                            new_embed_dict_master['wordsim'], \n",
    "                                            evalD.wordsim_df, \"SVR_Wordsim\",\n",
    "                                            comb_mode = False, SVC_or_SVR = 'SVR')\n",
    "print(\"Analysed SVR_Wordsim\")\n",
    "\n",
    "# RetrofittingProcedures.save_all_embeddings(new_embed_dict_master['wordsim'])\n",
    "\n",
    "# SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, \n",
    "#                                             new_embed_dict_master['wordsim'], \n",
    "#                                             evalD.wordsim_df, \"SVR_Wordsim\",\n",
    "#                                             comb_mode = True, SVC_or_SVR = 'SVR')\n",
    "# print(\"Analysed SVR_Wordsim combinatrics\")\n",
    "\n",
    "# new_embed_dict_master, responses_dict_master = {}, {}\n",
    "\n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "# # Wiki CS executions\n",
    "# new_embed_dict_master['wiki_cs'], responses_dict_master['wiki_cs'] = RetrofittingProcedures.execute_all_unsupervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, basis.neighbors_dict_master, \n",
    "#                                                           evalD.wiki_cs_df, \"wiki_cs_ind\")\n",
    "# print(\"Analysed wiki_cs_ind\")\n",
    "\n",
    "# SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master,\n",
    "#                                             new_embed_dict_master['wiki_cs'],\n",
    "#                                             evalD.wordsim_df, \"SVC_Wiki_CS\",\n",
    "#                                             comb_mode = False, SVC_or_SVR = 'SVC')\n",
    "# print(\"Analysed SVC_Wiki_CS\")\n",
    "\n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "# # Conceptnet executions\n",
    "# new_embed_dict_master['conceptnet'], responses_dict_master['conceptnet'] = RetrofittingProcedures.execute_all_unsupervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, basis.neighbors_dict_master, \n",
    "#                                                           evalD.concept_net_df, \"concept_net_ind\")\n",
    "# print(\"Analysed concept_net_ind\")\n",
    "\n",
    "# SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master,\n",
    "#                                             new_embed_dict_master['conceptnet'],\n",
    "#                                             evalD.wordsim_df, \"SVC_Conceptnet\",\n",
    "#                                             comb_mode = False, SVC_or_SVR = 'SVC')\n",
    "# print(\"Analysed SVC_Conceptnet\")\n",
    "\n",
    "# print(f\"Time taken for end-to-end execution: {time() - start_master_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-memory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hundred-virus",
   "metadata": {},
   "source": [
    "# Evaluation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioDF = pd.read_csv('../data/pedersen2007measures_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioDFNodesSet = set(bioDF.Term1_kg_id.to_list() + bioDF.Term2_kg_id.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "P279childParNodesSet = set(p279WordSimSeededDF_wabs_text.node1.to_list() + p279WordSimSeededDF_wabs_text.node2.to_list())\n",
    "P279siblingsNodesSet = set(p279Seeded_SiblingsDF3_wabs_text.node1.to_list() + p279Seeded_SiblingsDF3_wabs_text.node2.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(bioDF.Term1_kg_id.apply(lambda p: p in P279childParNodesSet or p in P279siblingsNodesSet)), \\\n",
    "sum(bioDF.Term2_kg_id.apply(lambda p: p in P279childParNodesSet or p in P279siblingsNodesSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "probaseNodesSet = set(probDF_Qnodes_DF_WQnodes1_subset.node1.to_list() + probDF_Qnodes_DF_WQnodes1_subset.node2.to_list())\n",
    "\n",
    "sum(bioDF.Term1_kg_id.apply(lambda p: p in probaseNodesSet)), \\\n",
    "sum(bioDF.Term2_kg_id.apply(lambda p: p in probaseNodesSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-civilian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-logic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-baker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-charm",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgtkEnv",
   "language": "python",
   "name": "kgtkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
