{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dynamic-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import combinations\n",
    "from math import comb\n",
    "from time import time\n",
    "from datetime import date\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "amazing-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDSIM_DF = '../data/evaluation/wordsim353_with_r3.csv'\n",
    "WORDSIM_OLD_FINAL_FILE = \"../data/evaluation/wordsim_old.csv\"\n",
    "DBPEDIA_MC_30_FINAL_FILE = \"../data/evaluation/mc-30_DBpedia.csv\"\n",
    "DBPEDIA_RG_65_FINAL_FILE = \"../data/evaluation/rg-65_DBpedia.csv\"\n",
    "\n",
    "CONCEPTNET_FILE = \"../data/evaluation/kgtk_conceptnet_final.csv\"\n",
    "WIKI_CS_FILE = '../data/evaluation/wikidata-cs_final.csv'\n",
    "\n",
    "INPUT_EMB_FOLDER_PATH = '../data/embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "improved-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basis\n",
    "P279_CHILD_PAR_DISTILBERT_COSSIM_FILE = \"../data/basis/P279_ChildPar.all-distilroberta-v1.csv\"\n",
    "P279_SIBLINGS_DISTILBERT_COSSIM_FILE = \"../data/basis/P279_Siblings.all-distilroberta-v1.csv\"\n",
    "\n",
    "P279_CHILD_PAR_CLASSSIM_FILE = \"../data/basis/P279_ChildPar.classSim.csv\"\n",
    "P279_SIBLINGS_CLASSSIM_FILE = \"../data/basis/P279_Siblings.classSim.csv\"\n",
    "\n",
    "PROBASE_FINAL_FILE = '../data/basis/intermediate_files/probase_WQnodes_subset_and_sim.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fallen-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDSIM_CLASS_SIM_FILE = '../data/embeddings/wordsim_class_sim.csv'\n",
    "WORDSIM_JC_SIM_FILE = '../data/embeddings/wordsim_jc_sim.csv'\n",
    "WORDSIM_TOP_SIM_FILE = '../data/embeddings/wordsim_top_sim.csv'\n",
    "\n",
    "WORDSIM_OLD_CLASS_SIM_FILE = '../data/embeddings/wordsim_old_class_sim.csv'\n",
    "WORDSIM_OLD_JC_SIM_FILE = '../data/embeddings/wordsim_old_jc_sim.csv'\n",
    "WORDSIM_OLD_TOP_SIM_FILE = '../data/embeddings/wordsim_old_top_sim.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-conspiracy",
   "metadata": {},
   "source": [
    "# Retrofitting Pre-Req Class Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-perception",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "arbitrary-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    This contains all the utility functions needed by any part of retrofitting\n",
    "    \"\"\"\n",
    "    _today = date.today()\n",
    "    today_date = _today.strftime(\"%b_%d_%Y\")\n",
    "    LABELS = ['I','M','U']\n",
    "    \n",
    "    @classmethod\n",
    "    def normalize(cls, embed_dict):\n",
    "        for key, val in embed_dict.items():\n",
    "            temp = np.array([float(val1) for val1 in val])\n",
    "            temp2 = temp**2\n",
    "            embed_dict[key] = temp / np.sqrt((temp2.sum() + 1e-6))\n",
    "        return embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def fetch_embeddings(cls, df):\n",
    "        embed_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            embed_dict[row.node] = row.value\n",
    "        return normalize(embed_dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def fill_coverage(cls, embed_dict, embed_name):\n",
    "        wordsim_df = pd.read_csv(WORDSIM_DF)\n",
    "#         wiki_cs_df = pd.read_csv(WIKICS_DF)\n",
    "#         concept_net_df = pd.read_csv(CONCEPTNET_DF)\n",
    "        \n",
    "        compulsory_coverage_set = set(\n",
    "                        wordsim_df['word1_kg_id'].to_list() \n",
    "                        + wordsim_df['word2_kg_id'].to_list())\n",
    "#                         + wiki_cs_df['word1_kg_id'].to_list() \n",
    "#                         + wiki_cs_df['word2_kg_id'].to_list()\n",
    "#                         + concept_net_df['word1_kg_id'].to_list()\n",
    "#                         + concept_net_df['word2_kg_id'].to_list())\n",
    "        \n",
    "        embed_size = len(embed_dict[next(iter(embed_dict))])\n",
    "        count = 0\n",
    "        for word in compulsory_coverage_set:\n",
    "            if word not in embed_dict:\n",
    "                embed_dict[word] = np.zeros((embed_size))\n",
    "                count += 1\n",
    "        print(f\"Added {count} corrections to {embed_name}\")\n",
    "        return embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def check_coverage(cls, embed_dict):\n",
    "        wordsim_df = pd.read_csv(WORDSIM_DF)\n",
    "        \n",
    "        compulsory_coverage_set = set(list(zip(wordsim_df['word1_kg_id'].to_list(), wordsim_df['word2_kg_id'].to_list())))\n",
    "        embed_size = len(embed_dict[next(iter(embed_dict))])\n",
    "        count = 0\n",
    "        for word1, word2 in compulsory_coverage_set:\n",
    "            if word1 not in embed_dict or word2 not in embed_dict:\n",
    "                count += 1\n",
    "        return (len(wordsim_df) - count)\n",
    "    \n",
    "    @classmethod\n",
    "    def find_missing_words(cls, embed_dict):\n",
    "        wordsim_df = pd.read_csv(WORDSIM_DF)\n",
    "        \n",
    "        compulsory_coverage_set = set((wordsim_df['word1_kg_id'].to_list()) + (wordsim_df['word2_kg_id'].to_list()))\n",
    "        embed_size = len(embed_dict[next(iter(embed_dict))])\n",
    "        missing_words = []\n",
    "        for word in compulsory_coverage_set:\n",
    "            if word not in embed_dict or not(embed_dict[word].any()):\n",
    "                missing_words.append(word)\n",
    "        return missing_words\n",
    "    \n",
    "    @classmethod\n",
    "    def determine_distances(cls, embed_dict, new_embed_dict):\n",
    "        dist = []\n",
    "        for word in embed_dict.keys():\n",
    "            dist.append(euclidean_distances([embed_dict[word]], [new_embed_dict[word]])[0][0])\n",
    "        return dist\n",
    "    \n",
    "    @classmethod\n",
    "    def serialize_embedding_dict(cls, embed_dict):\n",
    "        for key2 in embed_dict.keys():\n",
    "            embed_dict[key2] = embed_dict[key2].tolist() if type(embed_dict[key2]) != list else embed_dict[key2]\n",
    "        return embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def deserialize_embedding_dict(cls, embed_dict):\n",
    "        for key2 in embed_dict.keys():\n",
    "            embed_dict[key2] = np.array(embed_dict[key2])\n",
    "        return embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def label_samples(cls, score):\n",
    "        return 'I' if score <= 1.75 else 'U' if score >= 3.5 else 'M'\n",
    "    \n",
    "    @classmethod\n",
    "    def alt_label_samples(cls, score, quartiles):\n",
    "        return ['Q'+str(i+1) for i in range(len(quartiles) - 1) if quartiles[i] <= score < quartiles[i+1]][0]\n",
    "    \n",
    "    @classmethod\n",
    "    def alt2_label_samples(cls, row, quartiles):\n",
    "        return [i for i, quartile in (quartiles.items()) if (row.word1_kg_id, row.word2_kg_id) in quartile][0]\n",
    "    \n",
    "    @classmethod\n",
    "    def determine_cos_sim(cls, emb1, emb2):\n",
    "        return cosine_similarity(\n",
    "                np.array(emb1).reshape(1,-1), \n",
    "                np.array(emb2).reshape(1,-1)\n",
    "            )[0][0]\n",
    "    \n",
    "    @classmethod\n",
    "    def plot_confusion_matrix(cls, conf_matrix, title):\n",
    "        plt.close()\n",
    "        sns.heatmap(conf_matrix, xticklabels=Utils.LABELS, yticklabels=Utils.LABELS, annot=True)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(title+' Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "urban-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# def sizeof_fmt(num, suffix='B'):\n",
    "#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "#         if abs(num) < 1024.0:\n",
    "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "#         num /= 1024.0\n",
    "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "#     for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "#                              key= lambda x: -x[1])[:10]:\n",
    "#         print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-papua",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "excess-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings:\n",
    "    \"\"\"\n",
    "    Instance variables:\n",
    "        - embed_dict_master - holds all qnode to embedding mappings as a dictionary\n",
    "        - embedding_list - list of all keys of the above dictionary\n",
    "    \"\"\"\n",
    "    def __init__(self, has_embeddings_include: bool = True):\n",
    "        self.embed_dict_master = {}\n",
    "        self.emb_list = ['text_7_props', 'complex', 'transe', 'abstract_first_sent']\n",
    "        if has_embeddings_include: # TODO\n",
    "            self.emb_list += ['has_h', 'has_s']\n",
    "        self.embedding_lengths = {}\n",
    "        \n",
    "        for emb_key in tqdm(self.emb_list, desc='Input Embeddings', leave=False):\n",
    "            self.embed_dict_master[emb_key] = self.fetch_embedding(emb_key)\n",
    "            \n",
    "        self.fetch_embedding_stats()\n",
    "        print(\"Fetched all input embeddings\")\n",
    "\n",
    "    def fetch_embedding(self, emb_key):\n",
    "        emb = Utils.deserialize_embedding_dict(\n",
    "                json.load(open(INPUT_EMB_FOLDER_PATH+emb_key+'_orig_embedding_dict.json'))\n",
    "            )\n",
    "        print(f\"OG Coverage of {emb_key}: {Utils.check_coverage(emb)}\")\n",
    "        return Utils.fill_coverage(\n",
    "                emb, emb_key\n",
    "            )\n",
    "\n",
    "    def fetch_embedding_stats(self):\n",
    "        for emb_name in self.embed_dict_master.keys():\n",
    "            self.embedding_lengths[emb_name] = len(next(iter(self.embed_dict_master[emb_name].values())))\n",
    "            print(f\"Embedding: {emb_name}, Size: {len(self.embed_dict_master[emb_name].keys())}, Length: {self.embedding_lengths[emb_name]}\")\n",
    "\n",
    "class ReducedInputEmbeddings:\n",
    "    def __init__(self, embed_dict_master, final_embed_len):\n",
    "        self.embed_dict_master = copy.deepcopy(embed_dict_master)\n",
    "        self.final_embed_len = final_embed_len\n",
    "        for key in tqdm(self.embed_dict_master.keys()):\n",
    "#             tsne = TSNE(final_embed_len, verbose=1, method='exact')\n",
    "            tfmr = PCA(final_embed_len)\n",
    "            tfmr_proj = tfmr.fit_transform(pd.DataFrame(list(self.embed_dict_master[key].values())))\n",
    "            tfmr_proj = normalize(tfmr_proj, axis=0)\n",
    "            for w_key, emb in zip(self.embed_dict_master[key].keys(), tfmr_proj):\n",
    "                self.embed_dict_master[key][w_key] = emb        \n",
    "                \n",
    "    def generate_concatenated_embedding_dict(self, key_comb):\n",
    "        embedDict = defaultdict(list)\n",
    "        masterKeySet = set()\n",
    "        for key in key_comb:\n",
    "            for qnode in self.embed_dict_master[key]:\n",
    "                masterKeySet.add(qnode)\n",
    "        for qnode in masterKeySet:\n",
    "            for key in key_comb:\n",
    "                if qnode in self.embed_dict_master[key]:\n",
    "                    embedDict[qnode] = embedDict[qnode] + (self.embed_dict_master[key][qnode].tolist())\n",
    "                else:\n",
    "#                     print(\"Hit missing elem branch for concatenation\")\n",
    "                    embedDict[qnode] = embedDict[qnode] + [0]*self.final_embed_len\n",
    "            embedDict[qnode] = np.array(embedDict[qnode])\n",
    "        return dict(embedDict)\n",
    "\n",
    "class InputScoreTables:\n",
    "    def __init__(self, embed_dict_master, exception_cols, wordsim_new=True):\n",
    "        self.input_score_tables = {}\n",
    "        self.wordsim_new = wordsim_new\n",
    "        print(f\"Fetching {'new' if wordsim_new else 'old'} wordsim score tables and eval file\")\n",
    "        if wordsim_new:\n",
    "            self.input_score_tables['classSim'] = pd.read_csv(WORDSIM_CLASS_SIM_FILE)\n",
    "            self.input_score_tables['JC'] = pd.read_csv(WORDSIM_JC_SIM_FILE)\n",
    "            self.input_score_tables['topSim'] = pd.read_csv(WORDSIM_TOP_SIM_FILE)\n",
    "            self.wordsim = evalD.wordsim_df.copy()\n",
    "        else:\n",
    "            self.input_score_tables['classSim'] = pd.read_csv(WORDSIM_OLD_CLASS_SIM_FILE)\n",
    "            self.input_score_tables['JC'] = pd.read_csv(WORDSIM_OLD_JC_SIM_FILE)\n",
    "            self.input_score_tables['topSim'] = pd.read_csv(WORDSIM_OLD_TOP_SIM_FILE)\n",
    "            self.wordsim = evalD.old_wordsim_df.copy()\n",
    "            \n",
    "        self.input_score_tables['classSim']['embedding_na'] = self.input_score_tables['classSim']['embedding_cos_sim'].isna()\n",
    "        self.input_score_tables['classSim'] = self.input_score_tables['classSim'][self.input_score_tables['classSim'].word1_kg_id == self.input_score_tables['classSim'].word2_kg_id]\n",
    "        self.input_score_tables['JC']['embedding_na'] = self.input_score_tables['JC']['embedding_cos_sim'].isna()\n",
    "        self.input_score_tables['JC'] = self.input_score_tables['JC'][self.input_score_tables['JC'].word1_kg_id == self.input_score_tables['JC'].word2_kg_id]\n",
    "        self.input_score_tables['topSim']['embedding_na'] = self.input_score_tables['topSim']['embedding_cos_sim'].isna()\n",
    "        self.input_score_tables['topSim'] = self.input_score_tables['topSim'][self.input_score_tables['topSim'].word1_kg_id == self.input_score_tables['topSim'].word2_kg_id]\n",
    "        \n",
    "        if embed_dict_master is not None:\n",
    "            for emb in embed_dict_master:\n",
    "#                 print(f\"Emb: {emb}\")\n",
    "                self.input_score_tables[emb] = self.construct_wsim_tab(embed_dict_master[emb])\n",
    "        self.input_score_tables['average'] = self.get_averaged_dict(exception_cols)\n",
    "    \n",
    "    def construct_wsim_tab(self, embed_dict):\n",
    "        if self.wordsim_new:\n",
    "            eval_dataset = evalD.wordsim_df.copy()\n",
    "        else:\n",
    "            eval_dataset = evalD.old_wordsim_df.copy()\n",
    "\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset.apply(lambda p: Utils.determine_cos_sim(embed_dict[p['word1_kg_id']], embed_dict[p['word2_kg_id']]) \n",
    "                                                   if p['word1_kg_id'] in embed_dict and p['word2_kg_id'] in embed_dict \n",
    "                                                   else None, axis=1)\n",
    "        eval_dataset['embedding_na'] = eval_dataset['embedding_cos_sim'].isna()\n",
    "#         print(f\"Coverage: {len(eval_dataset) - eval_dataset['embedding_cos_sim'].isna().sum()}\")\n",
    "        eval_dataset['embedding_cos_sim'].fillna(eval_dataset['embedding_cos_sim'].mean(skipna=True), inplace=True)\n",
    "        \n",
    "        # Scale abs value of cosine similarities to 1,4 strictly\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset['embedding_cos_sim'].apply(lambda p: 4 - 3 * abs(p))\n",
    "        \n",
    "        return eval_dataset\n",
    "        \n",
    "    def get_pairwise_dict(self, tab_key):\n",
    "        if tab_key not in self.input_score_tables:\n",
    "            raise \"Key not present in table\"\n",
    "        return {(row['word1_kg_id'], row['word2_kg_id']): row['embedding_cos_sim'] for _, row in self.input_score_tables[tab_key].iterrows()}\n",
    "    \n",
    "    def get_averaged_dict(self, exception_cols):\n",
    "        print(f\"Returning averaged scores from {len(set(self.input_score_tables.keys()) - exception_cols)} algorithms - {set(self.input_score_tables.keys()) - exception_cols}\")\n",
    "        final_dict = defaultdict(list)\n",
    "        for tab_key in set(self.input_score_tables.keys()) - exception_cols:\n",
    "            for _, row in self.input_score_tables[tab_key].iterrows():\n",
    "                if row['embedding_na'] == False:\n",
    "                    final_dict[(row['word1_kg_id'], row['word2_kg_id'])].append(row['embedding_cos_sim'])\n",
    "                else:\n",
    "                    print('na embedding was present, hence skipped')\n",
    "        for key in final_dict:\n",
    "            final_dict[key] = np.mean(np.array(final_dict[key]))\n",
    "        \n",
    "        if self.wordsim_new:\n",
    "            eval_dataset = evalD.wordsim_df.copy()\n",
    "        else:\n",
    "            eval_dataset = evalD.old_wordsim_df.copy()\n",
    "\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset.apply(lambda p: final_dict[(p['word1_kg_id'], p['word2_kg_id'])], axis=1)\n",
    "        \n",
    "        return eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-secretary",
   "metadata": {},
   "source": [
    "## NeighborDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "detected-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborDatasets:\n",
    "    \"\"\"\n",
    "    Instance variables:\n",
    "        - neighbors_dict_master - holds all qnode to neighbor qnode mappings as a dictionary\n",
    "        - basis_list - list of all keys of the above dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_datasets_fetch: bool = False, probase_datasets_fetch: bool = True):\n",
    "        self.neighbors_dict_master = {}\n",
    "        \n",
    "        pbar = tqdm(desc='Neighbor Datasets', leave=False, total = \n",
    "                    3\n",
    "                    + (3 if class_datasets_fetch else 0) \n",
    "                    + (1 if probase_datasets_fetch else 0) \n",
    "                   )\n",
    "        \n",
    "        bert_P279_child_par_df = pd.read_csv(P279_CHILD_PAR_DISTILBERT_COSSIM_FILE)\n",
    "#         bert_P279_child_par_df_cross_enc = pd.read_csv('../data/Master_P279_dataset/P279ChildPar_filtered_cross_enc.csv')\n",
    "        bert_P279_siblings_df = pd.read_csv(P279_SIBLINGS_DISTILBERT_COSSIM_FILE)\n",
    "#         bert_P279_siblings_df_cross_enc = pd.read_csv('../data/Master_P279_dataset/P279Siblings_transP279_filtered_min_cols_with_desc_dups_removed_cross_enc.csv')\n",
    "        \n",
    "        self.neighbors_dict_master['bert_child_par'] = self.fetch_neighbours(bert_P279_child_par_df)\n",
    "        pbar.update(1)\n",
    "        self.neighbors_dict_master['bert_siblings'] = self.fetch_neighbours(bert_P279_siblings_df)\n",
    "        pbar.update(1)\n",
    "        self.neighbors_dict_master['bert_all'] = self.fetch_neighbours(pd.concat([\n",
    "                bert_P279_child_par_df, bert_P279_siblings_df\n",
    "            ]))\n",
    "        pbar.update(1)\n",
    "        \n",
    "#         self.neighbors_dict_master['cross_enc_child_par'] = self.fetch_neighbours(bert_P279_child_par_df)\n",
    "#         pbar.update(1)\n",
    "#         self.neighbors_dict_master['cross_enc_siblings'] = self.fetch_neighbours(bert_P279_siblings_df)\n",
    "#         pbar.update(1)\n",
    "#         self.neighbors_dict_master['cross_enc_all'] = self.fetch_neighbours(pd.concat([\n",
    "#                 bert_P279_child_par_df, bert_P279_siblings_df\n",
    "#             ]))\n",
    "#         pbar.update(1)\n",
    "            \n",
    "        if class_datasets_fetch:\n",
    "            class_P279_child_par_df = pd.read_csv(P279_CHILD_PAR_CLASSSIM_FILE)\n",
    "            class_P279_child_par_df['similarity_value'] = class_P279_child_par_df['classSim']\n",
    "            \n",
    "            class_P279_siblings_df = pd.read_csv(P279_SIBLINGS_CLASSSIM_FILE)\n",
    "            class_P279_siblings_df['similarity_value'] = class_P279_siblings_df['classSim']\n",
    "            \n",
    "            self.neighbors_dict_master['class_child_par'] = self.fetch_neighbours(class_P279_child_par_df)\n",
    "            pbar.update(1)\n",
    "            self.neighbors_dict_master['class_siblings'] = self.fetch_neighbours(class_P279_siblings_df)\n",
    "            pbar.update(1)\n",
    "            self.neighbors_dict_master['class_all'] = self.fetch_neighbours(pd.concat([\n",
    "                    class_P279_child_par_df, class_P279_siblings_df\n",
    "                ]))\n",
    "            pbar.update(1)\n",
    "\n",
    "        if probase_datasets_fetch:\n",
    "            probase_df = self.process_probase(PROBASE_FINAL_FILE)\n",
    "            \n",
    "            self.neighbors_dict_master['probase'] = self.fetch_neighbours(probase_df)\n",
    "            pbar.update(1)\n",
    "#             self.neighbors_dict_master['probase+bert_all'] = self.fetch_neighbours(pd.concat([\n",
    "#                     bert_P279_child_par_df, bert_P279_siblings_df, probase_df\n",
    "#                 ]))\n",
    "#             pbar.update(1)\n",
    "        \n",
    "        self.basis_list = list(self.neighbors_dict_master.keys())\n",
    "        \n",
    "        pbar.close()\n",
    "\n",
    "        print(f\"Fetched neighbour datasets: {self.basis_list}\")\n",
    "    \n",
    "    def process_probase(self, probase_file_path):\n",
    "        probase_df = pd.read_csv(probase_file_path)\n",
    "#         probase_df = probase_df.rename(columns={'n1_final_qnode': 'node1', 'n2_final_qnode': 'node2', 'sim': 'similarity_value'})\n",
    "        probase_df['similarity_value'] = 0.5 + 0.5 * probase_df['similarity_value']\n",
    "        \n",
    "        return probase_df\n",
    "        \n",
    "    def fetch_neighbours(self, df):\n",
    "        neighbours_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            if row.node1 not in neighbours_dict:\n",
    "                neighbours_dict[row.node1] = []\n",
    "            neighbours_dict[row.node1].append((row.node2, row.similarity_value))\n",
    "\n",
    "            if row.node2 not in neighbours_dict:\n",
    "                neighbours_dict[row.node2] = []\n",
    "            neighbours_dict[row.node2].append((row.node1, row.similarity_value))\n",
    "#         print(max([len(neigh) for neigh in neighbours_dict.values()]))\n",
    "        \n",
    "        return neighbours_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-service",
   "metadata": {},
   "source": [
    "## EvaluationDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "through-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationDatasets:\n",
    "    def __init__(self):\n",
    "        self.wordsim_df = pd.read_csv(WORDSIM_DF)\n",
    "        self.wordsim_df.category = self.wordsim_df.Avg.apply(Utils.label_samples)\n",
    "        self.fetch_distribution_stats(\"Wordsim-353\", self.wordsim_df)\n",
    "        \n",
    "        self.old_wordsim_df = pd.read_csv(WORDSIM_OLD_FINAL_FILE)\n",
    "        self.wordsim_df.category = self.wordsim_df.Avg.apply(Utils.label_samples)\n",
    "        self.fetch_distribution_stats(\"Wordsim-353 OLD\", self.old_wordsim_df)\n",
    "        \n",
    "        self.dbpedia_mc_30_df = pd.read_csv(DBPEDIA_MC_30_FINAL_FILE)\n",
    "        self.dbpedia_mc_30_df.category = self.dbpedia_mc_30_df.Avg.apply(Utils.label_samples)\n",
    "        self.fetch_distribution_stats(\"DBPedia MC 30\", self.dbpedia_mc_30_df)\n",
    "        \n",
    "        self.dbpedia_rg_65_df = pd.read_csv(DBPEDIA_RG_65_FINAL_FILE)\n",
    "        self.dbpedia_rg_65_df.category = self.dbpedia_rg_65_df.Avg.apply(Utils.label_samples)\n",
    "        self.fetch_distribution_stats(\"DBPedia RG 65\", self.dbpedia_rg_65_df)\n",
    "        \n",
    "#         self.wiki_cs_df = pd.read_csv('../data/wikidata-cs_categorized.csv')\n",
    "#         self.fetch_distribution_stats(\"Wikidata CS\", self.wiki_cs_df)\n",
    "        \n",
    "#         self.concept_net_df = pd.read_csv('../data/kgtk_conceptnet_evaluation.csv')\n",
    "#         self.fetch_distribution_stats(\"Concept Net\", self.concept_net_df)\n",
    "        \n",
    "        self.get_coverage_nodes()\n",
    "        \n",
    "    def fetch_distribution_stats(self, name, dataset):\n",
    "        print(f\"Dataset: {name}\")\n",
    "        print(dataset.category.value_counts())\n",
    "    \n",
    "    def get_coverage_nodes(self):\n",
    "        self.coverage = set(\n",
    "                        self.wordsim_df['word1_kg_id'].to_list() \n",
    "                        + self.wordsim_df['word2_kg_id'].to_list())\n",
    "#                         + self.wiki_cs_df['word1_kg_id'].to_list() \n",
    "#                         + self.wiki_cs_df['word2_kg_id'].to_list()\n",
    "#                         + self.concept_net_df['word1_kg_id'].to_list()\n",
    "#                         + self.concept_net_df['word2_kg_id'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-growth",
   "metadata": {},
   "source": [
    "## ResultMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "necessary-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultMetrics:\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_classification_results(cls,\n",
    "            embed_dict, \n",
    "            eval_dataset,\n",
    "            get_output_values: bool = False,\n",
    "            old_accuracy = None\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - embed_dict - dictionary of qnodes with node embeddings as its values\n",
    "            - eval_dataset - evaluation dataset as pandas dataframe that must have the \n",
    "                following columns for this function to work correctly:\n",
    "                * word1_kg_id - Qnode of node1 in the evaluation pair\n",
    "                * word2_kg_id - Qnode of node2 in the evaluation pair\n",
    "                * category - Category of the evaluation pair. One of the labels: I/U/M\n",
    "        Outputs:\n",
    "            - response_dict - Returns a dictionary with the following keys:\n",
    "                * covered_pairs - Indicates the number of pairs of the evaluation dataset that the \n",
    "                    embedding dictionary can cover\n",
    "                \n",
    "        \"\"\"\n",
    "        response_dict = {}\n",
    "        \n",
    "        eval_dataset = eval_dataset.copy()\n",
    "\n",
    "        missing_words_set = set(\n",
    "            eval_dataset[eval_dataset.word1_kg_id.apply(lambda p: p not in embed_dict)].word1_kg_id.to_list() \n",
    "            + eval_dataset[eval_dataset.word2_kg_id.apply(lambda p: p not in embed_dict)].word2_kg_id.to_list()\n",
    "        )\n",
    "        \n",
    "        response_dict['covered_pairs'] = len(eval_dataset)\n",
    "\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset.apply(lambda p: Utils.determine_cos_sim(embed_dict[p['word1_kg_id']], embed_dict[p['word2_kg_id']]) \n",
    "                                                   if p['word1_kg_id'] in embed_dict and p['word2_kg_id'] in embed_dict \n",
    "                                                   else None, axis=1)\n",
    "        \n",
    "        eval_dataset['embedding_cos_sim'].fillna(eval_dataset['embedding_cos_sim'].mean(skipna=True), inplace=True)\n",
    "        \n",
    "        # Scale abs value of cosine similarities to 1,4 strictly\n",
    "        eval_dataset['embedding_cos_sim'] = eval_dataset['embedding_cos_sim'].apply(lambda p: 4 - 3 * abs(p))\n",
    "\n",
    "        response_dict['accuracy'] = 100 * accuracy_score(\n",
    "                eval_dataset['category'],\n",
    "                eval_dataset['embedding_cos_sim'].apply(Utils.label_samples)\n",
    "            )\n",
    "    \n",
    "        response_dict['classification_report'] = classification_report(\n",
    "                eval_dataset['category'], \n",
    "                eval_dataset['embedding_cos_sim'].apply(Utils.label_samples), \n",
    "                output_dict=True\n",
    "            )\n",
    "\n",
    "        response_dict['conf_matrix'] = confusion_matrix(\n",
    "                eval_dataset['category'], \n",
    "                eval_dataset['embedding_cos_sim'].apply(Utils.label_samples), \n",
    "                labels=Utils.LABELS\n",
    "            )\n",
    "        if 'Avg' in eval_dataset.columns:\n",
    "            response_dict['KT'] = stats.kendalltau(eval_dataset['Avg'], eval_dataset['embedding_cos_sim']).correlation\n",
    "            response_dict['SR'] = stats.spearmanr(eval_dataset['Avg'], eval_dataset['embedding_cos_sim']).correlation\n",
    "            response_dict['RMSE'] = mean_squared_error(eval_dataset['Avg'], eval_dataset['embedding_cos_sim'], squared=False)\n",
    "        else:\n",
    "            response_dict['KT'] = None\n",
    "            response_dict['SR'] = None\n",
    "            response_dict['RMSE'] = None\n",
    "        \n",
    "        if old_accuracy is not None:\n",
    "            response_dict['increase_acc'] = response_dict['accuracy'] - old_accuracy\n",
    "        else:\n",
    "            response_dict['increase_acc'] = None\n",
    "        \n",
    "        if get_output_values:\n",
    "            response_dict['preds'] = eval_dataset['embedding_cos_sim'].apply(Utils.label_samples)\n",
    "\n",
    "        return response_dict, \\\n",
    "                (response_dict['covered_pairs'],  \\\n",
    "                 response_dict['accuracy'], \\\n",
    "                 response_dict['increase_acc'], \\\n",
    "                 \n",
    "                 response_dict['classification_report']['I']['precision'],  \\\n",
    "                 response_dict['classification_report']['I']['recall'],  \\\n",
    "                 response_dict['classification_report']['I']['f1-score'],   \\\n",
    "                 \n",
    "                 response_dict['classification_report']['M']['precision'],  \\\n",
    "                 response_dict['classification_report']['M']['recall'],  \\\n",
    "                 response_dict['classification_report']['M']['f1-score'], \\\n",
    "                 \n",
    "                 response_dict['classification_report']['U']['precision'],  \\\n",
    "                 response_dict['classification_report']['U']['recall'],  \\\n",
    "                 response_dict['classification_report']['U']['f1-score'], \\\n",
    "                \n",
    "                 response_dict['KT'], \\\n",
    "                 response_dict['SR'],\n",
    "                 response_dict['RMSE'])\n",
    "    \n",
    "    @classmethod\n",
    "    def fetch_best_result_for_emb(cls, results_df, emb_col, target_col, iter_col, highest: bool = True):\n",
    "        opt_value = {}\n",
    "        for _, row in results_df.iterrows():\n",
    "            if row[emb_col] not in opt_value:\n",
    "                opt_value[row[emb_col]] = {'opt_metric': float('-inf') if highest else float('inf'),\n",
    "                                     'opt_row': [], 'old_row': []}\n",
    "            if row[iter_col] == 0:\n",
    "                opt_value[row[emb_col]]['old_row'] = row\n",
    "            else:\n",
    "                if (highest and row[target_col] > opt_value[row[emb_col]]['opt_metric']) \\\n",
    "                        or (not(highest) and row[target_col] < opt_value[row[emb_col]]['opt_metric']):\n",
    "                    opt_value[row[emb_col]]['opt_metric'] = row[target_col]\n",
    "                    opt_value[row[emb_col]]['opt_row'] = row\n",
    "        best_results = []\n",
    "        for emb_key in opt_value:\n",
    "            best_results.append(opt_value[emb_key]['old_row'])\n",
    "            best_results.append(opt_value[emb_key]['opt_row'])\n",
    "        return pd.DataFrame(best_results, columns = results_df.columns)\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_classification_n_regression_stats(cls, ist, suffix, standard_labels=True):\n",
    "#         q_size = len(evalD.wordsim_df) // 4\n",
    "        if not(standard_labels):\n",
    "#             print(f\"At most {q_size} rows in each quartile\")\n",
    "#             temp_wordsim_df = ist.wordsim.sort_values(by=['Avg', 'word1_kg_id', 'word2_kg_id'])\n",
    "#             quantile_sets = {'Q'+str(i+1): set(temp_wordsim_df[q_size*i:q_size*(i+1)].apply(lambda p: (p.word1_kg_id, p.word2_kg_id), axis=1).to_list()) \n",
    "#                                  for i in range(4) }\n",
    "#             wordsim_cats = ist.wordsim.apply(Utils.alt2_label_samples, args=(quantile_sets,), axis=1)\n",
    "            quantiles = evalD.wordsim_df.Avg.quantile([0, 0.25, 0.5, 0.75, 1]).to_list()\n",
    "            quantiles[0], quantiles[-1] = float('-inf'), float('inf')\n",
    "            print(f\"Quantiles being used: {quantile_sets}\")\n",
    "            wordsim_cats = ist.wordsim.apply(Utils.alt_label_samples, args=(quantiles,))\n",
    "        else:\n",
    "            wordsim_cats = ist.wordsim.category\n",
    "        \n",
    "        eval_df = ist.wordsim.copy()\n",
    "        if not(standard_labels):\n",
    "#             eval_df['quartile'] = eval_df.apply(Utils.alt2_label_samples, args=(quantile_sets,), axis=1)\n",
    "            eval_df['quartile'] = eval_df.apply(Utils.alt_label_samples, args=(quantiles,))\n",
    "            \n",
    "        results = []\n",
    "        for tab_key in ist.input_score_tables:\n",
    "            cosSimPreds_df = ist.input_score_tables[tab_key]\n",
    "            response_dict = {}\n",
    "            if standard_labels:\n",
    "                preds = cosSimPreds_df['embedding_cos_sim'].apply(Utils.label_samples)\n",
    "            else:\n",
    "                temp_wordsim_df = cosSimPreds_df.sort_values(by=['embedding_cos_sim', 'word1_kg_id', 'word2_kg_id'])\n",
    "                quantile_sets = {'Q'+str(i+1): set(temp_wordsim_df[q_size*i:q_size*(i+1)].apply(lambda p: (p.word1_kg_id, p.word2_kg_id), axis=1).to_list()) \n",
    "                                 for i in range(4) }\n",
    "                preds = cosSimPreds_df.apply(Utils.alt_label_samples, args=(quantiles,))\n",
    "            eval_df[tab_key] = cosSimPreds_df['embedding_cos_sim']\n",
    "            eval_df[tab_key+'_cat'] = preds\n",
    "            response_dict['accuracy'] = 100 * accuracy_score(\n",
    "                    wordsim_cats,\n",
    "                    preds\n",
    "                )\n",
    "\n",
    "            response_dict['classification_report'] = classification_report(\n",
    "                    wordsim_cats,\n",
    "                    preds, \n",
    "                    output_dict=True\n",
    "                )\n",
    "\n",
    "            response_dict['KT'] = stats.kendalltau(cosSimPreds_df['Avg'], cosSimPreds_df['embedding_cos_sim']).correlation\n",
    "            response_dict['SR'] = stats.spearmanr(cosSimPreds_df['Avg'], cosSimPreds_df['embedding_cos_sim']).correlation\n",
    "            response_dict['RMSE'] = mean_squared_error(cosSimPreds_df['Avg'], cosSimPreds_df['embedding_cos_sim'], squared=False)\n",
    "            \n",
    "            # SVR related stats\n",
    "            temp_dict = {'basis': '', 'emb': tab_key, 'weightedness': True, \n",
    "                'iter_num': 0, 'weight_case': None, 'svm_input': 'score'}\n",
    "            svr_res = SVMProcedures.execute_supervised_scenario(\n",
    "                evalD.wordsim_df, temp_dict, ist.get_pairwise_dict(tab_key), \n",
    "                {},num_of_splits=10,\n",
    "                comb_mode=False, SVC_or_SVR='SVR', score_table_mode=True\n",
    "            )\n",
    "            \n",
    "            if standard_labels:\n",
    "                results.append([tab_key, response_dict['accuracy'], response_dict['classification_report']['macro avg']['precision'], \\\n",
    "                            response_dict['classification_report']['macro avg']['recall'], \\\n",
    "                            response_dict['classification_report']['macro avg']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['I']['precision'], \\\n",
    "                            response_dict['classification_report']['I']['recall'], \\\n",
    "                            response_dict['classification_report']['I']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['M']['precision'], \\\n",
    "                            response_dict['classification_report']['M']['recall'], \\\n",
    "                            response_dict['classification_report']['M']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['U']['precision'], \\\n",
    "                            response_dict['classification_report']['U']['recall'], \\\n",
    "                            response_dict['classification_report']['U']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['KT'], response_dict['SR'], response_dict['RMSE'], svr_res[-2], svr_res[-1], svr_res[-3]])\n",
    "            else:\n",
    "                results.append([tab_key, response_dict['accuracy'], response_dict['classification_report']['macro avg']['precision'], \\\n",
    "                            response_dict['classification_report']['macro avg']['recall'], \\\n",
    "                            response_dict['classification_report']['macro avg']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['Q1']['precision'], \\\n",
    "                            response_dict['classification_report']['Q1']['recall'], \\\n",
    "                            response_dict['classification_report']['Q1']['f1-score'], \\\n",
    "                            response_dict['classification_report']['Q1']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['Q2']['precision'], \\\n",
    "                            response_dict['classification_report']['Q2']['recall'], \\\n",
    "                            response_dict['classification_report']['Q2']['f1-score'], \\\n",
    "                            response_dict['classification_report']['Q2']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['Q3']['precision'], \\\n",
    "                            response_dict['classification_report']['Q3']['recall'], \\\n",
    "                            response_dict['classification_report']['Q3']['f1-score'], \\\n",
    "                            response_dict['classification_report']['Q3']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['classification_report']['Q4']['precision'], \\\n",
    "                            response_dict['classification_report']['Q4']['recall'], \\\n",
    "                            response_dict['classification_report']['Q4']['f1-score'], \\\n",
    "                            response_dict['classification_report']['Q4']['f1-score'], \\\n",
    "                                \\\n",
    "                            response_dict['KT'], response_dict['SR'], response_dict['RMSE'], svr_res[-2], svr_res[-1], svr_res[-3]])\n",
    "        if standard_labels:\n",
    "            res_df = pd.DataFrame(results, columns=['algorithm', 'accuracy', 'P', 'R', 'F1', 'I P', 'I R', 'I F1', 'M P', 'M R', 'M F1', 'U P', 'U R', 'U F1', 'Kendall Tau', 'Spearman Rank', 'RMSE', 'SVR Kendall Tau', 'SVR Spearman Rank', 'SVR RMSE'])\n",
    "        else:\n",
    "            res_df = pd.DataFrame(results, columns=['algorithm', 'accuracy', 'P', 'R', 'F1', 'Q1 P', 'Q1 R', 'Q1 F1', 'Q2 P', 'Q2 R', 'Q2 F1', 'Q3 P', 'Q3 R', 'Q3 F1', 'Q4 P', 'Q4 R', 'Q4 F1', 'Kendall Tau', 'Spearman Rank', 'RMSE', 'SVR Kendall Tau', 'SVR Spearman Rank', 'SVR RMSE'])\n",
    "        res_df.to_csv('../data/retrofitting/score_table_algorithms_results.' + suffix + '.' + Utils.today_date + '.csv', index=False)\n",
    "        return res_df, eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-cosmetic",
   "metadata": {},
   "source": [
    "## RetrofittingProcedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prescription-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrofittingProcedures:\n",
    "    \n",
    "    np_label_samples = np.vectorize(Utils.label_samples)\n",
    "    \n",
    "    @classmethod\n",
    "    def retrofit(cls,embed_dict, neighbors_dict, weight_case, weight_assignment=False):\n",
    "        new_embed_dict = {}\n",
    "        for word in embed_dict.keys():\n",
    "            if word in neighbors_dict:\n",
    "                neighbs = neighbors_dict[word]\n",
    "                neighbs = list(filter(lambda p: p[0] in embed_dict, neighbs))\n",
    "                if len(neighbs) == 0:\n",
    "                    new_embed_dict[word] = embed_dict[word]\n",
    "                    continue\n",
    "                if weight_assignment:\n",
    "                    sum_of_sims = sum([neighb[1] for neighb in neighbs])\n",
    "                    sum_of_embs = sum([embed_dict[neighb[0]] * float(neighb[1]) for neighb in neighbs])\n",
    "                else:\n",
    "                    sum_of_sims = sum([1 for neighb in neighbs])\n",
    "                    sum_of_embs = sum([embed_dict[neighb[0]] for neighb in neighbs])\n",
    "\n",
    "                if weight_case == 1:\n",
    "                    new_embed_dict[word] = (embed_dict[word] * (len(neighbs)) + sum_of_embs) / ((len(neighbs)) + sum_of_sims)\n",
    "                elif weight_case == 2:\n",
    "                    new_embed_dict[word] = (embed_dict[word] * (len(neighbs))**2 + sum_of_embs) / ((len(neighbs))**2 + sum_of_sims)\n",
    "                elif weight_case == 0.5:\n",
    "                    new_embed_dict[word] = (embed_dict[word] * (len(neighbs))**0.5 + sum_of_embs) / ((len(neighbs))**0.5 + sum_of_sims)\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                new_embed_dict[word] = embed_dict[word]\n",
    "        return new_embed_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def execute_all_unsupervised_scenarios(cls,\n",
    "                emb_list, basis_list, \n",
    "                embed_dict_master, neigh_dict_master,\n",
    "                eval_dataset,\n",
    "                scenario_name: str,\n",
    "                num_of_iterations: int = 2, \n",
    "                weightedness_list: list = [True],\n",
    "                weight_cases_list: list = [1],\n",
    "                get_output_values: bool = False\n",
    "            ):\n",
    "        \n",
    "        new_embed_dict_master = {}\n",
    "        responses_dict_master = {}\n",
    "        results = []\n",
    "        \n",
    "        for basis in tqdm(basis_list, desc='Basis', leave=False):\n",
    "            for emb in tqdm(emb_list, desc='Embedding', leave=False):\n",
    "                for weightedness in weightedness_list:\n",
    "                    for weight_case in tqdm(weight_cases_list, desc='Weight Case', leave=False):\n",
    "                        # Base Reference Initializations and Calculations\n",
    "                        embed_dict = embed_dict_master[emb]\n",
    "                        responses_dict, result_values = ResultMetrics.compute_classification_results(\n",
    "                            embed_dict, eval_dataset, get_output_values=get_output_values, old_accuracy=None)\n",
    "                        results.append([emb, basis, weight_case, weightedness, 0, 'base', *result_values, 0])\n",
    "                        old_accuracy = responses_dict['accuracy']\n",
    "                        \n",
    "                        for iter_num in tqdm(range(1,num_of_iterations+1), desc='Iteration', leave=False):\n",
    "                            start_time = time()\n",
    "                            \n",
    "                            case_name = emb + '_' + basis + '_' + str(weight_case) + ('_weighted' if weightedness else '_unweighted')\n",
    "                            \n",
    "                            new_embed_dict = cls.retrofit(embed_dict, neigh_dict_master[basis], weight_case, weightedness)\n",
    "                            \n",
    "                            responses_dict, result_values = ResultMetrics.compute_classification_results(\n",
    "                                new_embed_dict, eval_dataset, get_output_values=get_output_values, old_accuracy=old_accuracy)\n",
    "                            \n",
    "                            results.append([emb, basis, weight_case, weightedness, iter_num, case_name, \\\n",
    "                                                *result_values, \\\n",
    "                                                time() - start_time\n",
    "                                            ])\n",
    "                            \n",
    "                            new_embed_dict_master[case_name] = embed_dict = new_embed_dict\n",
    "                            responses_dict_master[case_name] = responses_dict\n",
    "\n",
    "        #                     if iter_num == num_of_iterations and highestOne:\n",
    "        #                         case_name = gR[0] + '_' + gR[1] + '_' + str(gR[2]) + '_weighted'\n",
    "        #                         new_embed_dict_master[case_name] = serializeEmbeddingDict(new_embed_dict_master[case_name])\n",
    "        #                         highestOne = False\n",
    "        #                         json.dump(new_embed_dict_master[case_name],open('../data/Master_P279_dataset/embeddings/new_embedding_dict_'+case_name+'.json','w'))\n",
    "        #                         new_embed_dict_master[case_name] = deserializeEmbeddingDict(new_embed_dict_master[case_name])\n",
    "#         print(results)\n",
    "#         ['text_7_props', 'bert_child_par', 1, True, 0, 'base', 344, 56.68604651162791, None, 0.30952380952380953, \n",
    "#          0.65, 0.41935483870967744, 0.6289752650176679, 0.8054298642533937, 0.7063492063492064, \n",
    "#          0.21052631578947367, 0.038834951456310676, 0.06557377049180328, 0.31127513538205615, 0.4132700578622946]\n",
    "        resultsDF = pd.DataFrame(results, columns=['Embedding', 'Basis', 'Weight Case', 'Weightedness', \n",
    "                                                   'Iteration Num', 'Case Name', \\\n",
    "                                                   'No. of Pairs Covered', 'Accuracy', 'Increase in Accuracy', \\\n",
    "                                                   'I Precision', 'I Recall', 'I F1-Score', \\\n",
    "                                                   'M Precision', 'M Recall', 'M F1-Score', \\\n",
    "                                                   'U Precision', 'U Recall', 'U F1-Score',\n",
    "                                                   'KT Correlation', 'SpearmanR Correlation', 'RMSE', \\\n",
    "                                                   'Time to Retrofit'])\n",
    "        resultsDF.to_csv('../data/retrofitting/retro_unsup_results.' + scenario_name + '.'+ Utils.today_date +'.csv', index=False)\n",
    "#         best_results_df = ResultMetrics.fetch_best_result_for_emb(resultsDF, 'Embedding', 'Accuracy', 'Iteration Num', highest=True)\n",
    "#         best_results_df.to_csv('../data/retrofitting/retro_unsup_results.' + scenario_name + '.'+ Utils.today_date +'.best.csv', index=False)\n",
    "        \n",
    "#         cls.save_needed_embeddings(new_embed_dict_master)\n",
    "        \n",
    "        return new_embed_dict_master, responses_dict_master\n",
    "    \n",
    "    @classmethod\n",
    "    def save_all_embeddings(cls, new_embed_dict_master):\n",
    "        for case_name in new_embed_dict_master:\n",
    "            json.dump(Utils.serialize_embedding_dict(new_embed_dict_master[case_name]), open(INPUT_EMB_FOLDER_PATH + 'new_embeddings/' + case_name + '.' + Utils.today_date + '.json', 'w'))\n",
    "    \n",
    "    @classmethod\n",
    "    def save_needed_embeddings(cls, new_embed_dict_master):\n",
    "        for case_name in new_embed_dict_master:\n",
    "            temp = {key: new_embed_dict_master[case_name][key] for key in new_embed_dict_master[case_name] if key in evalD.coverage}\n",
    "            new_embed_dict_master[case_name] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-implementation",
   "metadata": {},
   "source": [
    "## SVMProcedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "missing-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMProcedures:\n",
    "    @classmethod\n",
    "    def execute_supervised_scenario(cls,\n",
    "                eval_dataset, case, embed_dict_master, new_embed_dict_master, \n",
    "                num_of_splits = 10, \n",
    "                comb_mode: bool = False, SVC_or_SVR: str = 'SVC', \n",
    "                score_table_mode: bool = False\n",
    "            ):\n",
    "        \n",
    "        X = []        \n",
    "        \n",
    "        ################\n",
    "        # 2 Approaches based on argument: `comb_mode`\n",
    "        ################\n",
    "        \n",
    "        if comb_mode: ########## COMBINATION MODE CODE ####################\n",
    "            case_name = \" & \".join(case['emb']) + '_' + case['basis'] + '_' + str(case['weight_case']) + ('_weighted' if case['weightedness'] else '_unweighted')\n",
    "            \n",
    "            for _, row in eval_dataset.iterrows():\n",
    "                if case['svm_input'] == 'emb':\n",
    "                    tempX = []\n",
    "                    for individual_emb in case['emb']:\n",
    "                        ind_case_name = individual_emb + '_' + case['basis'] + '_' + str(case['weight_case']) + ('_weighted' if case['weightedness'] else '_unweighted')\n",
    "                        if case['iter_num'] != 0 and ind_case_name not in new_embed_dict_master:\n",
    "                            return case_name, case, None\n",
    "                        if case['iter_num'] == 0:\n",
    "                            if score_table_mode:\n",
    "                                raise \"Not yet implemented\"\n",
    "                            tempX += embed_dict_master[individual_emb][row['word1_kg_id']].tolist() + embedDictMaster[individual_emb][row['word2_kg_id']].tolist()\n",
    "                        else:\n",
    "                            tempX += new_embed_dict_master[ind_case_name][row['word1_kg_id']].tolist() + newEmbedDictMaster[ind_case_name][row['word2_kg_id']].tolist()\n",
    "                    X.append(tempX)\n",
    "                else:\n",
    "                    tempX = []\n",
    "                    for individual_emb in case['emb']:\n",
    "                        ind_case_name = individual_emb + '_' + case['basis'] + '_' + str(case['weight_case']) + ('_weighted' if case['weightedness'] else '_unweighted')\n",
    "                        if case['iter_num'] != 0 and ind_case_name not in new_embed_dict_master:\n",
    "                            return case_name, case, None\n",
    "                        if case['iter_num'] == 0:\n",
    "                            if score_table_mode:\n",
    "                                raise \"Not yet implemented\"\n",
    "                            tempX.append(abs(Utils.determine_cos_sim(\n",
    "                                embed_dict_master[individual_emb][row['word1_kg_id']], \n",
    "                                embed_dict_master[individual_emb][row['word2_kg_id']]\n",
    "                            )))\n",
    "                        else:\n",
    "                            tempX.append(abs(Utils.determine_cos_sim(\n",
    "                                new_embed_dict_master[ind_case_name][row['word1_kg_id']],\n",
    "                                new_embed_dict_master[ind_case_name][row['word2_kg_id']]\n",
    "                            )))\n",
    "                    X.append(tempX)\n",
    "\n",
    "        else: ########## NON-COMBINATION MODE CODE ####################\n",
    "            case_name = case['emb'] + '_' + case['basis'] + '_' + str(case['weight_case']) + ('_weighted' if case['weightedness'] else '_unweighted')\n",
    "            if case['iter_num'] != 0 and case_name not in new_embed_dict_master:\n",
    "                return case_name, case, None\n",
    "            for _, row in eval_dataset.iterrows():\n",
    "                if case['svm_input'] == 'emb':\n",
    "                    if case['iter_num'] == 0:\n",
    "                        if score_table_mode:\n",
    "                            raise \"Not yet implemented\"\n",
    "                        X.append(embed_dict_master[case['emb']][row['word1_kg_id']].tolist() + embed_dict_master[case['emb']][row['word2_kg_id']].tolist())\n",
    "                    else:\n",
    "                        X.append(new_embed_dict_master[case_name][row['word1_kg_id']].tolist() + new_embed_dict_master[case_name][row['word2_kg_id']].tolist())\n",
    "                else:\n",
    "                    if case['iter_num'] == 0:\n",
    "                        if score_table_mode:\n",
    "                            X.append(embed_dict_master[(row['word1_kg_id'], row['word2_kg_id'])])\n",
    "                        else:\n",
    "                            X.append(abs(Utils.determine_cos_sim(\n",
    "                                    embed_dict_master[case['emb']][row['word1_kg_id']], \n",
    "                                    embed_dict_master[case['emb']][row['word2_kg_id']]\n",
    "                                )))\n",
    "                    else:\n",
    "                        X.append(abs(Utils.determine_cos_sim(\n",
    "                                new_embed_dict_master[case_name][row['word1_kg_id']],\n",
    "                                new_embed_dict_master[case_name][row['word2_kg_id']]\n",
    "                            )))\n",
    "                    \n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        ################\n",
    "        # 2 Approaches based on argument: `SVC_or_SVR`\n",
    "        ################\n",
    "        \n",
    "        # Target split depending on SVC or SVM\n",
    "        if SVC_or_SVR == 'SVC':\n",
    "            Y = eval_dataset['category']\n",
    "        elif SVC_or_SVR == 'SVR':\n",
    "            if 'Avg' not in eval_dataset.columns:\n",
    "                raise ValueError(\"Avg column not present in the provided eval_dataset\")\n",
    "            Y = (eval_dataset['Avg'] - 1) / 3\n",
    "        else:\n",
    "            raise ValueError(\"Invalid SVC_or_SVR provided\")\n",
    "        \n",
    "        if SVC_or_SVR == 'SVC':\n",
    "            skf = StratifiedKFold(n_splits=num_of_splits, random_state=19, shuffle=True)\n",
    "            X_train_splits, X_test_splits, Y_train_splits, Y_test_splits = [], [], [], []\n",
    "            for train_index, test_index in skf.split(X, Y):\n",
    "                X_train_splits.append(X.iloc[train_index])\n",
    "                X_test_splits.append(X.iloc[test_index])\n",
    "                Y_train_splits.append(Y.iloc[train_index])\n",
    "                Y_test_splits.append(Y.iloc[test_index])\n",
    "        elif SVC_or_SVR == 'SVR':\n",
    "            skf = KFold(n_splits=num_of_splits, random_state=19, shuffle=True)\n",
    "            X_train_splits, X_test_splits, Y_train_splits, Y_test_splits = [], [], [], []\n",
    "            for train_index, test_index in skf.split(X, Y):\n",
    "                X_train_splits.append(X.iloc[train_index])\n",
    "                X_test_splits.append(X.iloc[test_index])\n",
    "                Y_train_splits.append(Y.iloc[train_index])\n",
    "                Y_test_splits.append(Y.iloc[test_index])\n",
    "\n",
    "        preds = []\n",
    "        \n",
    "        # Classifier/Regressor training depending on SVC or SVM\n",
    "        if SVC_or_SVR == 'SVC':\n",
    "            for X_train1, Y_train1, X_test1, Y_test1 in zip(X_train_splits, Y_train_splits, X_test_splits, Y_test_splits):\n",
    "                clf = make_pipeline(StandardScaler(), SVC(gamma='auto', random_state=100, max_iter=100))\n",
    "                clf.fit(X_train1, Y_train1)\n",
    "                preds.append(clf.predict(X_test1))\n",
    "                \n",
    "            acc = 0\n",
    "            for pred, Y_test1 in zip(preds, Y_test_splits):\n",
    "                acc += accuracy_score(pred, Y_test1)\n",
    "\n",
    "            return case_name, *list(case.values()), acc/num_of_splits\n",
    "        \n",
    "        elif SVC_or_SVR == 'SVR':\n",
    "            for X_train1, Y_train1, X_test1, Y_test1 in zip(X_train_splits, Y_train_splits, X_test_splits, Y_test_splits):\n",
    "                clf = make_pipeline(StandardScaler(), SVR(gamma='auto', max_iter=100))\n",
    "                clf.fit(X_train1, Y_train1)\n",
    "                preds.append(clf.predict(X_test1))\n",
    "            \n",
    "            acc = 0\n",
    "            ktCorr = 0\n",
    "            spearmanR = 0\n",
    "            for pred, Y_test1 in zip(preds, Y_test_splits):\n",
    "                acc += mean_squared_error(pred * 3 + 1, Y_test1 * 3 + 1, squared=False)\n",
    "                ktCorr += stats.kendalltau(Y_test1 * 3 + 1, pred * 3 + 1).correlation\n",
    "                spearmanR += stats.spearmanr(Y_test1 * 3 + 1, pred * 3 + 1).correlation\n",
    "                \n",
    "            return case_name, *list(case.values()), acc/num_of_splits, ktCorr/num_of_splits, spearmanR/num_of_splits\n",
    "    \n",
    "    @classmethod\n",
    "    def execute_all_supervised_scenarios(cls,\n",
    "                emb_list, basis_list, embed_dict_master, new_embed_dict_master, \n",
    "                eval_dataset, \n",
    "                scenario_name: str,\n",
    "                num_of_splits = 10,\n",
    "                comb_mode: bool = False, SVC_or_SVR: str = 'SVC', \n",
    "                num_of_iterations = 2,\n",
    "                num_of_jobs = 1\n",
    "            ):\n",
    "        \n",
    "        if not(comb_mode):\n",
    "            svm_cases_list = []\n",
    "            for basis in basis_list:\n",
    "                for emb in emb_list:\n",
    "                    for weightedness in [True]:\n",
    "                        for iter_num in range(0,num_of_iterations+1):\n",
    "                            for weight_case in [1]:\n",
    "                                for svm_input in ['score']:\n",
    "                                    temp_dict = {'basis': basis, 'emb': emb, 'weightedness': weightedness, \n",
    "                                                'iter_num': iter_num, 'weight_case': weight_case, 'svm_input': svm_input}\n",
    "                                    svm_cases_list.append(temp_dict) \n",
    "        else:\n",
    "            svm_cases_list = []\n",
    "            for basis in basis_list:\n",
    "                for emb in emb_list:\n",
    "                    for weightedness in [True]:\n",
    "                        for iter_num in range(0,num_of_iterations+1):\n",
    "                            for weight_case in [1]:\n",
    "                                for svm_input in ['score']:\n",
    "                                    for i in range(1,len(emb_list)+1):\n",
    "                                        for emb_comb in combinations(emb_list, i):\n",
    "                                            temp_dict = {'basis': basis, 'emb': emb_comb, 'weightedness': weightedness, \n",
    "                                                        'iter_num': iter_num, 'weight_case': weight_case, 'svm_input': svm_input}\n",
    "                                            svm_cases_list.append(temp_dict) \n",
    "\n",
    "        results = Parallel(n_jobs=num_of_jobs)(delayed(cls.execute_supervised_scenario)(\n",
    "                eval_dataset, caseDict, embed_dict_master, \n",
    "                new_embed_dict_master,num_of_splits,\n",
    "                comb_mode, SVC_or_SVR\n",
    "            ) for caseDict in tqdm(svm_cases_list))\n",
    "        \n",
    "        if SVC_or_SVR == 'SVC':\n",
    "            results_df = pd.DataFrame(results, columns=['Case Name','Basis','Embedding','Weightedness', 'Iteration Num', 'Weight Case', 'Technique','Accuracy'])\n",
    "#             best_results_df = ResultMetrics.fetch_best_result_for_emb(results_df, 'Embedding', 'Accuracy', 'Iteration Num', highest=True)\n",
    "#             best_results_df.to_csv('../data/retrofitting/retro_SVC_results.' + scenario_name + '.'+ Utils.today_date +'.best.csv', index=False)\n",
    "        else:\n",
    "            results_df = pd.DataFrame(results, columns=['Case Name','Basis','Embedding','Weightedness', 'Iteration Num', 'Weight Case', 'Technique','MSE', 'KT Correlation', 'SR Correlation'])\n",
    "#             best_results_df = ResultMetrics.fetch_best_result_for_emb(results_df, 'Embedding', 'MSE', 'Iteration Num', highest=False)\n",
    "#             best_results_df.to_csv('../data/retrofitting/retro_SVR_results.' + scenario_name + '.'+ Utils.today_date +'.best.csv', index=False)\n",
    "            \n",
    "        results_df.to_csv('../data/retrofitting/retro_SVM_results.' + scenario_name + '.'+ Utils.today_date +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-voluntary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "retained-frederick",
   "metadata": {},
   "source": [
    "# The Master Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "partial-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>ID</th>\n",
       "      <th>H_Sim</th>\n",
       "      <th>H_Dim</th>\n",
       "      <th>F_Sim</th>\n",
       "      <th>F_Dim</th>\n",
       "      <th>N_Sim</th>\n",
       "      <th>N_Dim</th>\n",
       "      <th>...</th>\n",
       "      <th>D_Dim</th>\n",
       "      <th>P_Sim</th>\n",
       "      <th>P_Dim</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Stdev</th>\n",
       "      <th>H_orig</th>\n",
       "      <th>H_reversed</th>\n",
       "      <th>word1_kg_id</th>\n",
       "      <th>word2_kg_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>197</td>\n",
       "      <td>money</td>\n",
       "      <td>property</td>\n",
       "      <td>195</td>\n",
       "      <td>2</td>\n",
       "      <td>I</td>\n",
       "      <td>3</td>\n",
       "      <td>I</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>s</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.707</td>\n",
       "      <td>5.063</td>\n",
       "      <td>4.938</td>\n",
       "      <td>Q1368</td>\n",
       "      <td>Q1400881</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index Word 1    Word 2   ID  H_Sim H_Dim  F_Sim F_Dim  N_Sim N_Dim  ...  \\\n",
       "191    197  money  property  195      2     I      3     I      3     D  ...   \n",
       "\n",
       "     D_Dim P_Sim  P_Dim  Avg  Stdev  H_orig  H_reversed  word1_kg_id  \\\n",
       "191    NaN   3.0      s  3.0  0.707   5.063       4.938        Q1368   \n",
       "\n",
       "    word2_kg_id category  \n",
       "191    Q1400881        M  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ist.wordsim[(ist.wordsim.word1_kg_id + ist.wordsim.word2_kg_id).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "sitting-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalD.wordsim_df = evalD.wordsim_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "vocal-baghdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q42602',\n",
       " 'Q10998',\n",
       " 'Q12897867',\n",
       " 'Q7377',\n",
       " 'Q19939',\n",
       " 'Q35694',\n",
       " 'Q105427573',\n",
       " 'Q5113',\n",
       " 'Q25365',\n",
       " 'Q729',\n",
       " 'Q25472932']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utils.find_missing_words(inp.embed_dict_master['text_7_props'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "departmental-purpose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ist.input_score_tables['text_7_props'].embedding_cos_sim.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "married-click",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evalD.wordsim_df.Avg.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-lambda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-treat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-welsh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-breast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "south-strand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Input Embeddings:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Coverage of text_7_props: 326\n",
      "Added 11 corrections to text_7_props\n",
      "OG Coverage of complex: 344\n",
      "Added 0 corrections to complex\n",
      "OG Coverage of transe: 344\n",
      "Added 0 corrections to transe\n",
      "OG Coverage of abstract_first_sent: 344\n",
      "Added 0 corrections to abstract_first_sent\n",
      "OG Coverage of has_h: 337\n",
      "Added 7 corrections to has_h\n",
      "OG Coverage of has_a: 20\n",
      "Added 372 corrections to has_a\n",
      "OG Coverage of has_s: 234\n",
      "Added 93 corrections to has_s\n",
      "Embedding: text_7_props, Size: 220011, Length: 1024\n",
      "Embedding: complex, Size: 241694, Length: 100\n",
      "Embedding: transe, Size: 241694, Length: 100\n",
      "Embedding: abstract_first_sent, Size: 241694, Length: 768\n",
      "Embedding: has_h, Size: 166208, Length: 200\n",
      "Embedding: has_a, Size: 28247, Length: 200\n",
      "Embedding: has_s, Size: 117083, Length: 200\n",
      "Fetched all input embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Neighbor Datasets:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched neighbour datasets: ['bert_child_par', 'bert_siblings', 'bert_all', 'probase']\n",
      "Dataset: Wordsim-353\n",
      "M    221\n",
      "U    103\n",
      "I     20\n",
      "Name: category, dtype: int64\n",
      "Dataset: Wordsim-353 OLD\n",
      "M    280\n",
      "U     44\n",
      "I     25\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "new_embed_dict_master, responses_dict_master = {}, {}\n",
    "\n",
    "# # Load all supporting files\n",
    "inp = InputEmbeddings()\n",
    "basis = NeighborDatasets()\n",
    "evalD = EvaluationDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "prepared-browse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc6736ccf91434f8da7382b7c679bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8913c48d8097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minp_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReducedInputEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dict_master\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconc_emb_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp_tsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_concatenated_embedding_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dict_master\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a6c737dbf45b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dict_master, final_embed_len)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#             tsne = TSNE(final_embed_len, verbose=1, method='exact')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtfmr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_embed_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtfmr_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dict_master\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mtfmr_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfmr_proj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dict_master\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfmr_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    568\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;31m# last ditch effort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_convert_object_array\u001b[0;34m(content, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"O\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inp_tsne = ReducedInputEmbeddings(inp.embed_dict_master, 100)\n",
    "conc_emb_dict = inp_tsne.generate_concatenated_embedding_dict(list(set(inp.embed_dict_master.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "circular-monte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covered_pairs': 349,\n",
       " 'accuracy': 44.69914040114613,\n",
       " 'classification_report': {'I': {'precision': 0.8888888888888888,\n",
       "   'recall': 0.32,\n",
       "   'f1-score': 0.47058823529411764,\n",
       "   'support': 25},\n",
       "  'M': {'precision': 0.8320610687022901,\n",
       "   'recall': 0.3892857142857143,\n",
       "   'f1-score': 0.5304136253041363,\n",
       "   'support': 280},\n",
       "  'U': {'precision': 0.18660287081339713,\n",
       "   'recall': 0.8863636363636364,\n",
       "   'f1-score': 0.30830039525691694,\n",
       "   'support': 44},\n",
       "  'accuracy': 0.4469914040114613,\n",
       "  'macro avg': {'precision': 0.6358509428015253,\n",
       "   'recall': 0.531883116883117,\n",
       "   'f1-score': 0.436434085285057,\n",
       "   'support': 349},\n",
       "  'weighted avg': {'precision': 0.7547560108156244,\n",
       "   'recall': 0.4469914040114613,\n",
       "   'f1-score': 0.49812532481035937,\n",
       "   'support': 349}},\n",
       " 'conf_matrix': array([[  8,  17,   0],\n",
       "        [  1, 109, 170],\n",
       "        [  0,   5,  39]]),\n",
       " 'KT': 0.4188226957013864,\n",
       " 'SR': 0.5855565621170609,\n",
       " 'RMSE': 0.8370554555481824,\n",
       " 'increase_acc': None}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_dict, _ = ResultMetrics.compute_classification_results(\n",
    "                            conc_emb_dict, evalD.old_wordsim_df, get_output_values=False, old_accuracy=None)\n",
    "responses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "freelance-daughter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covered_pairs': 344,\n",
       " 'accuracy': 55.81395348837209,\n",
       " 'classification_report': {'I': {'precision': 1.0,\n",
       "   'recall': 0.45,\n",
       "   'f1-score': 0.6206896551724138,\n",
       "   'support': 20},\n",
       "  'M': {'precision': 0.7777777777777778,\n",
       "   'recall': 0.4434389140271493,\n",
       "   'f1-score': 0.5648414985590777,\n",
       "   'support': 221},\n",
       "  'U': {'precision': 0.40669856459330145,\n",
       "   'recall': 0.8252427184466019,\n",
       "   'f1-score': 0.5448717948717948,\n",
       "   'support': 103},\n",
       "  'accuracy': 0.5581395348837209,\n",
       "  'macro avg': {'precision': 0.7281587807903597,\n",
       "   'recall': 0.5728938774912504,\n",
       "   'f1-score': 0.5768009828677622,\n",
       "   'support': 344},\n",
       "  'weighted avg': {'precision': 0.6795896541918574,\n",
       "   'recall': 0.5581395348837209,\n",
       "   'f1-score': 0.5621091835953469,\n",
       "   'support': 344}},\n",
       " 'conf_matrix': array([[  9,  10,   1],\n",
       "        [  0,  98, 123],\n",
       "        [  0,  18,  85]]),\n",
       " 'KT': 0.42603956544144383,\n",
       " 'SR': 0.5644769083815114,\n",
       " 'RMSE': 0.6932338182205002,\n",
       " 'increase_acc': None}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_dict, _ = ResultMetrics.compute_classification_results(\n",
    "                            conc_emb_dict, evalD.wordsim_df, get_output_values=False, old_accuracy=None)\n",
    "responses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "important-pollution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching new wordsim score tables and eval file\n",
      "Returning averaged scores from 9 algorithms - {'text_7_props', 'topSim', 'has_s', 'has_h', 'abstract_first_sent', 'classSim', 'complex', 'transe', 'JC'}\n",
      "At most 83 rows in each quartile\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-7115967a8c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputScoreTables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dict_master\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResultMetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_classification_n_regression_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wsim_quantiles'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mres_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/retrofitting/wordsim_quantile_analysis.'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday_date\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-271795aae360>\u001b[0m in \u001b[0;36mcompute_classification_n_regression_stats\u001b[0;34m(cls, ist, suffix, standard_labels)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m#             quantiles[0], quantiles[-1] = float('-inf'), float('inf')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m#             print(f\"Quantiles being used: {quantile_sets}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mwordsim_cats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordsim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malt2_label_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantile_sets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mwordsim_cats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordsim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7763\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7764\u001b[0m         )\n\u001b[0;32m-> 7765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgtkEnv/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-7bc5aeedb34f>\u001b[0m in \u001b[0;36malt2_label_samples\u001b[0;34m(cls, row, quartiles)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malt2_label_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquartiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquartile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquartiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword1_kg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2_kg_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquartile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ist = InputScoreTables(inp.embed_dict_master, [])\n",
    "_,res_df = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim_quantiles', standard_labels=False)\n",
    "res_df.to_csv('../data/retrofitting/wordsim_quantile_analysis.'+ Utils.today_date +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "satellite-discovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['classSim', 'JC', 'topSim', 'text_7_props', 'complex', 'transe', 'abstract_first_sent', 'has_h', 'has_a', 'has_s', 'average'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ist.input_score_tables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cardiac-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching new wordsim score tables and eval file\n",
      "Returning averaged scores from 9 algorithms - {'text_7_props', 'topSim', 'has_s', 'has_h', 'abstract_first_sent', 'classSim', 'complex', 'transe', 'JC'}\n"
     ]
    }
   ],
   "source": [
    "ist = InputScoreTables(inp.embed_dict_master, [])\n",
    "_,res_df = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim_orig', standard_labels=True)\n",
    "res_df.to_csv('../data/retrofitting/wordsim_all_algo_scores.'+ Utils.today_date +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "governmental-yeast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>ID</th>\n",
       "      <th>H_Sim</th>\n",
       "      <th>H_Dim</th>\n",
       "      <th>F_Sim</th>\n",
       "      <th>F_Dim</th>\n",
       "      <th>N_Sim</th>\n",
       "      <th>N_Dim</th>\n",
       "      <th>D_Sim</th>\n",
       "      <th>...</th>\n",
       "      <th>abstract_first_sent</th>\n",
       "      <th>abstract_first_sent_cat</th>\n",
       "      <th>has_h</th>\n",
       "      <th>has_h_cat</th>\n",
       "      <th>has_a</th>\n",
       "      <th>has_a_cat</th>\n",
       "      <th>has_s</th>\n",
       "      <th>has_s_cat</th>\n",
       "      <th>average</th>\n",
       "      <th>average_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>cup</td>\n",
       "      <td>article</td>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.958764</td>\n",
       "      <td>U</td>\n",
       "      <td>2.872906</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.482037</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>dollar</td>\n",
       "      <td>loss</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.999627</td>\n",
       "      <td>U</td>\n",
       "      <td>2.698693</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.256196</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>dollar</td>\n",
       "      <td>profit</td>\n",
       "      <td>93</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>U</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.932936</td>\n",
       "      <td>U</td>\n",
       "      <td>2.713157</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.864850</td>\n",
       "      <td>U</td>\n",
       "      <td>3.175763</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>game</td>\n",
       "      <td>series</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.944594</td>\n",
       "      <td>U</td>\n",
       "      <td>3.050802</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.153498</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>glass</td>\n",
       "      <td>magician</td>\n",
       "      <td>131</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.988100</td>\n",
       "      <td>U</td>\n",
       "      <td>3.684964</td>\n",
       "      <td>U</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.368972</td>\n",
       "      <td>M</td>\n",
       "      <td>3.457468</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>lad</td>\n",
       "      <td>wizard</td>\n",
       "      <td>157</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.958942</td>\n",
       "      <td>U</td>\n",
       "      <td>3.162558</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.647433</td>\n",
       "      <td>U</td>\n",
       "      <td>3.481917</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>line</td>\n",
       "      <td>insurance</td>\n",
       "      <td>164</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>I</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.990768</td>\n",
       "      <td>U</td>\n",
       "      <td>3.564862</td>\n",
       "      <td>U</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.405219</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>lover</td>\n",
       "      <td>quarrel</td>\n",
       "      <td>170</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>U</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.956682</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.336049</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>ministry</td>\n",
       "      <td>culture</td>\n",
       "      <td>185</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.923127</td>\n",
       "      <td>U</td>\n",
       "      <td>2.928390</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.639812</td>\n",
       "      <td>U</td>\n",
       "      <td>3.331104</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>214</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.965832</td>\n",
       "      <td>U</td>\n",
       "      <td>3.191536</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.321735</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>opera</td>\n",
       "      <td>industry</td>\n",
       "      <td>219</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.909644</td>\n",
       "      <td>U</td>\n",
       "      <td>3.366666</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.854761</td>\n",
       "      <td>U</td>\n",
       "      <td>3.275512</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>peace</td>\n",
       "      <td>insurance</td>\n",
       "      <td>222</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>I</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.947423</td>\n",
       "      <td>U</td>\n",
       "      <td>3.259685</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.921533</td>\n",
       "      <td>U</td>\n",
       "      <td>3.331159</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>possibility</td>\n",
       "      <td>girl</td>\n",
       "      <td>236</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.982334</td>\n",
       "      <td>U</td>\n",
       "      <td>3.345150</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.549929</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>problem</td>\n",
       "      <td>airport</td>\n",
       "      <td>248</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.980484</td>\n",
       "      <td>U</td>\n",
       "      <td>3.283850</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.295501</td>\n",
       "      <td>M</td>\n",
       "      <td>3.494283</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>production</td>\n",
       "      <td>hike</td>\n",
       "      <td>251</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.909545</td>\n",
       "      <td>U</td>\n",
       "      <td>2.876798</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>252</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.957576</td>\n",
       "      <td>U</td>\n",
       "      <td>3.526570</td>\n",
       "      <td>U</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.846789</td>\n",
       "      <td>U</td>\n",
       "      <td>3.625227</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>psychology</td>\n",
       "      <td>clinic</td>\n",
       "      <td>257</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.914253</td>\n",
       "      <td>U</td>\n",
       "      <td>3.126622</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.859934</td>\n",
       "      <td>U</td>\n",
       "      <td>3.394986</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>reason</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>269</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.914178</td>\n",
       "      <td>U</td>\n",
       "      <td>3.289672</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.317394</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>sign</td>\n",
       "      <td>recess</td>\n",
       "      <td>284</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.962186</td>\n",
       "      <td>U</td>\n",
       "      <td>3.402407</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.400176</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>size</td>\n",
       "      <td>prominence</td>\n",
       "      <td>287</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.988955</td>\n",
       "      <td>U</td>\n",
       "      <td>3.371083</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>U</td>\n",
       "      <td>3.336471</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>stock</td>\n",
       "      <td>jaguar</td>\n",
       "      <td>298</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.962147</td>\n",
       "      <td>U</td>\n",
       "      <td>3.639395</td>\n",
       "      <td>U</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.890572</td>\n",
       "      <td>U</td>\n",
       "      <td>3.593627</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>stock</td>\n",
       "      <td>life</td>\n",
       "      <td>299</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.956109</td>\n",
       "      <td>U</td>\n",
       "      <td>2.821933</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.969930</td>\n",
       "      <td>U</td>\n",
       "      <td>3.391226</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>street</td>\n",
       "      <td>children</td>\n",
       "      <td>305</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.990232</td>\n",
       "      <td>U</td>\n",
       "      <td>3.430664</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "      <td>U</td>\n",
       "      <td>3.077853</td>\n",
       "      <td>M</td>\n",
       "      <td>3.384957</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows  42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word 1        Word 2   ID  H_Sim H_Dim  F_Sim F_Dim  N_Sim N_Dim  \\\n",
       "72           cup       article   61      4   NaN      4   NaN      4   NaN   \n",
       "103       dollar          loss   92      3     D      3     D      4   NaN   \n",
       "104       dollar        profit   93      3     D      3     U      3     D   \n",
       "135         game        series  126      3     H      3     H      3     H   \n",
       "140        glass      magician  131      4   NaN      4   NaN      4   NaN   \n",
       "162          lad        wizard  157      4   NaN      4   NaN      4   NaN   \n",
       "169         line     insurance  164      4   NaN      3     I      4   NaN   \n",
       "175        lover       quarrel  170      3     D      3     U      4   NaN   \n",
       "187     ministry       culture  185      4   NaN      3     D      4   NaN   \n",
       "216         noon        string  214      4   NaN      4   NaN      4   NaN   \n",
       "219        opera      industry  219      4   NaN      4   NaN      4   NaN   \n",
       "222        peace     insurance  222      4   NaN      3     I      4   NaN   \n",
       "236  possibility          girl  236      4   NaN      4   NaN      4   NaN   \n",
       "248      problem       airport  248      4   NaN      4   NaN      4   NaN   \n",
       "251   production          hike  251      4   NaN      4   NaN      4   NaN   \n",
       "252    professor      cucumber  252      4   NaN      4   NaN      4   NaN   \n",
       "258   psychology        clinic  257      3     D      3     L      4   NaN   \n",
       "269       reason  hypertension  269      4   NaN      4   NaN      4   NaN   \n",
       "284         sign        recess  284      4   NaN      4   NaN      4   NaN   \n",
       "287         size    prominence  287      2     S      2     S      4   NaN   \n",
       "298        stock        jaguar  298      4   NaN      4   NaN      4   NaN   \n",
       "299        stock          life  299      4   NaN      4   NaN      4   NaN   \n",
       "305       street      children  305      4   NaN      3     H      4   NaN   \n",
       "\n",
       "     D_Sim  ... abstract_first_sent  abstract_first_sent_cat     has_h  \\\n",
       "72       4  ...            3.958764                        U  2.872906   \n",
       "103      4  ...            3.999627                        U  2.698693   \n",
       "104      4  ...            3.932936                        U  2.713157   \n",
       "135      4  ...            3.944594                        U  3.050802   \n",
       "140      4  ...            3.988100                        U  3.684964   \n",
       "162      4  ...            3.958942                        U  3.162558   \n",
       "169      4  ...            3.990768                        U  3.564862   \n",
       "175      3  ...            3.956682                        U  4.000000   \n",
       "187      4  ...            3.923127                        U  2.928390   \n",
       "216      4  ...            3.965832                        U  3.191536   \n",
       "219      4  ...            3.909644                        U  3.366666   \n",
       "222      4  ...            3.947423                        U  3.259685   \n",
       "236      4  ...            3.982334                        U  3.345150   \n",
       "248      4  ...            3.980484                        U  3.283850   \n",
       "251      4  ...            3.909545                        U  2.876798   \n",
       "252      4  ...            3.957576                        U  3.526570   \n",
       "258      3  ...            3.914253                        U  3.126622   \n",
       "269      4  ...            3.914178                        U  3.289672   \n",
       "284      4  ...            3.962186                        U  3.402407   \n",
       "287      3  ...            3.988955                        U  3.371083   \n",
       "298      4  ...            3.962147                        U  3.639395   \n",
       "299      4  ...            3.956109                        U  2.821933   \n",
       "305      4  ...            3.990232                        U  3.430664   \n",
       "\n",
       "     has_h_cat  has_a  has_a_cat     has_s has_s_cat   average average_cat  \n",
       "72           M    4.0          U  4.000000         U  3.482037           M  \n",
       "103          M    4.0          U  4.000000         U  3.256196           M  \n",
       "104          M    4.0          U  3.864850         U  3.175763           M  \n",
       "135          M    4.0          U  4.000000         U  3.153498           M  \n",
       "140          U    4.0          U  3.368972         M  3.457468           M  \n",
       "162          M    4.0          U  3.647433         U  3.481917           M  \n",
       "169          U    4.0          U  4.000000         U  3.405219           M  \n",
       "175          U    4.0          U  4.000000         U  3.336049           M  \n",
       "187          M    4.0          U  3.639812         U  3.331104           M  \n",
       "216          M    4.0          U  4.000000         U  3.321735           M  \n",
       "219          M    4.0          U  3.854761         U  3.275512           M  \n",
       "222          M    4.0          U  3.921533         U  3.331159           M  \n",
       "236          M    4.0          U  4.000000         U  3.549929           U  \n",
       "248          M    4.0          U  3.295501         M  3.494283           M  \n",
       "251          M    4.0          U  4.000000         U  3.188762           M  \n",
       "252          U    4.0          U  3.846789         U  3.625227           U  \n",
       "258          M    4.0          U  3.859934         U  3.394986           M  \n",
       "269          M    4.0          U  4.000000         U  3.317394           M  \n",
       "284          M    4.0          U  4.000000         U  3.400176           M  \n",
       "287          M    4.0          U  4.000000         U  3.336471           M  \n",
       "298          U    4.0          U  3.890572         U  3.593627           U  \n",
       "299          M    4.0          U  3.969930         U  3.391226           M  \n",
       "305          M    4.0          U  3.077853         M  3.384957           M  \n",
       "\n",
       "[23 rows x 42 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[res_df.abstract_first_sent >= 3.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "ist = InputScoreTables(inp.embed_dict_master, set(['has_a']), False)\n",
    "_,res_df = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim_old', standard_labels=True)\n",
    "res_df.to_csv('../data/retrofitting/wordsim_all_algo_scores.wsim_old.'+ Utils.today_date +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "neither-ambassador",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacde2b5f6af48f69309a4534c6e7796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysed SVR_Wordsim\n",
      "CPU times: user 22.1 s, sys: 401 ms, total: 22.5 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_master_time = time()\n",
    "\n",
    "\n",
    "# Wordsim executions\n",
    "# new_embed_dict_master['wordsim'], responses_dict_master['wordsim'] = RetrofittingProcedures.execute_all_unsupervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, basis.neighbors_dict_master, \n",
    "#                                                                             evalD.wordsim_df, \"wordsim_ind\")\n",
    "\n",
    "# # ist = InputScoreTables(inp.embed_dict_master)\n",
    "# # _ = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim')\n",
    "\n",
    "# # ist = InputScoreTables(inp.embed_dict_master, False)\n",
    "# # _ = ResultMetrics.compute_classification_n_regression_stats(ist, 'wsim_old')\n",
    "\n",
    "# print(\"Analysed wordsim_ind\")\n",
    "\n",
    "# SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, \n",
    "#                                             new_embed_dict_master['wordsim'], \n",
    "#                                             evalD.wordsim_df, \"SVC_Wordsim\",\n",
    "#                                             comb_mode = False, SVC_or_SVR = 'SVC')\n",
    "# print(\"Analysed SVC_Wordsim\")\n",
    "\n",
    "SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, \n",
    "                                            new_embed_dict_master['wordsim'], \n",
    "                                            evalD.wordsim_df, \"SVR_Wordsim\",\n",
    "                                            comb_mode = False, SVC_or_SVR = 'SVR')\n",
    "print(\"Analysed SVR_Wordsim\")\n",
    "\n",
    "# RetrofittingProcedures.save_all_embeddings(new_embed_dict_master['wordsim'])\n",
    "\n",
    "# SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, \n",
    "#                                             new_embed_dict_master['wordsim'], \n",
    "#                                             evalD.wordsim_df, \"SVR_Wordsim\",\n",
    "#                                             comb_mode = True, SVC_or_SVR = 'SVR')\n",
    "# print(\"Analysed SVR_Wordsim combinatrics\")\n",
    "\n",
    "# new_embed_dict_master, responses_dict_master = {}, {}\n",
    "\n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "# # Wiki CS executions\n",
    "# new_embed_dict_master['wiki_cs'], responses_dict_master['wiki_cs'] = RetrofittingProcedures.execute_all_unsupervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, basis.neighbors_dict_master, \n",
    "#                                                           evalD.wiki_cs_df, \"wiki_cs_ind\")\n",
    "# print(\"Analysed wiki_cs_ind\")\n",
    "\n",
    "# SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master,\n",
    "#                                             new_embed_dict_master['wiki_cs'],\n",
    "#                                             evalD.wordsim_df, \"SVC_Wiki_CS\",\n",
    "#                                             comb_mode = False, SVC_or_SVR = 'SVC')\n",
    "# print(\"Analysed SVC_Wiki_CS\")\n",
    "\n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "                                                                                                                               \n",
    "# # Conceptnet executions\n",
    "# new_embed_dict_master['conceptnet'], responses_dict_master['conceptnet'] = RetrofittingProcedures.execute_all_unsupervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master, basis.neighbors_dict_master, \n",
    "#                                                           evalD.concept_net_df, \"concept_net_ind\")\n",
    "# print(\"Analysed concept_net_ind\")\n",
    "\n",
    "# SVMProcedures.execute_all_supervised_scenarios(inp.emb_list, basis.basis_list, inp.embed_dict_master,\n",
    "#                                             new_embed_dict_master['conceptnet'],\n",
    "#                                             evalD.wordsim_df, \"SVC_Conceptnet\",\n",
    "#                                             comb_mode = False, SVC_or_SVR = 'SVC')\n",
    "# print(\"Analysed SVC_Conceptnet\")\n",
    "\n",
    "# print(f\"Time taken for end-to-end execution: {time() - start_master_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-memory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hundred-virus",
   "metadata": {},
   "source": [
    "# Evaluation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioDF = pd.read_csv('../data/pedersen2007measures_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioDFNodesSet = set(bioDF.Term1_kg_id.to_list() + bioDF.Term2_kg_id.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "P279childParNodesSet = set(p279WordSimSeededDF_wabs_text.node1.to_list() + p279WordSimSeededDF_wabs_text.node2.to_list())\n",
    "P279siblingsNodesSet = set(p279Seeded_SiblingsDF3_wabs_text.node1.to_list() + p279Seeded_SiblingsDF3_wabs_text.node2.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(bioDF.Term1_kg_id.apply(lambda p: p in P279childParNodesSet or p in P279siblingsNodesSet)), \\\n",
    "sum(bioDF.Term2_kg_id.apply(lambda p: p in P279childParNodesSet or p in P279siblingsNodesSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "probaseNodesSet = set(probDF_Qnodes_DF_WQnodes1_subset.node1.to_list() + probDF_Qnodes_DF_WQnodes1_subset.node2.to_list())\n",
    "\n",
    "sum(bioDF.Term1_kg_id.apply(lambda p: p in probaseNodesSet)), \\\n",
    "sum(bioDF.Term2_kg_id.apply(lambda p: p in probaseNodesSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-civilian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-logic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-baker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-charm",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgtkEnv",
   "language": "python",
   "name": "kgtkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
